{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nemanovich/LLM-essentials/blob/main/advanced_rag_components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n"
      ],
      "metadata": {
        "id": "qQh7ewhdqZFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG components\n",
        "\n",
        "In this notebook, we'll discuss:\n",
        "\n",
        "* Two-stage RAG pipeline that augments retrieval with **reranking**\n",
        "* **Graph databases** which allow to capture connections inside data better than vector stores do\n",
        "* **Hierarchical Navigable Small World** (**HNSW**) which is the nearest neighbour search algorithm powering many of today's vector stores."
      ],
      "metadata": {
        "id": "QIgO73GPq22_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "WqCgRtIRIcN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = userdata.get('NEBIUS_API_KEY')"
      ],
      "metadata": {
        "id": "NRpRGdl5IdJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-stage retrieval, take 1: Reranking\n",
        "\n",
        "RAG inherited the idea of two-step retrieval from web search and recommendations.\n",
        "\n",
        "The problem with vector search is that, despite everything, distance between embeddings may fail to fully capture semantic similarity. Model-based relevance scorers might potentially perform better. At the same time, vector search boils down to computing scalar products of vectors, which is significantly cheaper than computing `score(query, database_entry`) for all entries of our database.\n",
        "\n",
        "Inspired by these considerations, the following process is often uses:\n",
        "\n",
        "1. Use cheap vector retrieval to get a large set of candidates (larger than we need)\n",
        "2. Score these candidates with a model, which is a more powerful but at the same time more expensive scorer. These models are often known as **rerankers**.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mrk83CENtZHTzkqEOBI-frCj4lo7sBSR\" width=600 />\n",
        "\n",
        "[Source](https://www.pinecone.io/learn/series/rag/rerankers/)\n",
        "\n",
        "</center>\n",
        "\n",
        "A popular reranker architecture are **cross encoders** - encoder-only transformers that take a concatenation of two sequences as an input and produce a relevance score\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=14SpSRIyyY_xfrxR4XHkIPitWj5zQTDKz\" width=400 />\n",
        "\n",
        "</center>\n",
        "\n",
        "However, recently LLMs are also being used as rerankers."
      ],
      "metadata": {
        "id": "RNFbIV-wFVZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As examples, we'll use two models from [Mixedbread](https://www.mixedbread.com/) which is a service providing RAG tools including a very good reranker model family.\n",
        "\n",
        "The models we'll try are:\n",
        "\n",
        "1. **mxbai-rerank-large-v1** ([its page on Hugging Face](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)), which is a 435M-size cross-encoder model with context length of 512 tokens - small enough for quick experimentation.\n",
        "\n",
        "  See more details in Mixedbread's [blog post](https://www.mixedbread.com/blog/mxbai-rerank-v1).\n",
        "\n",
        "2. **mxbai-rerank-base-v2** ([its page on Hugging Face](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v2)), which is an LLM fine tuned from **Qwen-2.5-0.5B**.\n",
        "\n",
        "  The **mxbai-rerank-base-v2** model and its larger counterpart **mxbai-rerank-large-v2** are also interesting due to their training methodology. The authors took the 0.5B and 1.5B version of **Qwen-2.5** as a base model and used a three-stage training process which is quite characteristic for Q1 2025:\n",
        "\n",
        "  ![](https://www.mixedbread.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftraining-methodology.be8e6c37.png&w=1920&q=75&dpl=dpl_7psJGN4mGSVvEewvaP5Y3yiVRQmP)\n",
        "\n",
        "  [Source](https://www.mixedbread.com/blog/mxbai-rerank-v2)\n",
        "\n",
        "  See more details in [their blog post](https://www.mixedbread.com/blog/mxbai-rerank-v2).\n",
        "\n",
        "Due to their small size, these models may be used on CPU, though we'd recommend using GPU for **mxbai-rerank-base-v2**."
      ],
      "metadata": {
        "id": "xveKznt0Q340"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying mxbai-rerank-base-v1**\n",
        "\n",
        "For this simple demonstration, we created 7 documents - some of them tell about the effect of the Elixir of Umbral Sight, while others are here to confuse the reranker. All o them might be fetched by vector search."
      ],
      "metadata": {
        "id": "Y937q7FQWuD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load the model\n",
        "reranker_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n",
        "\n",
        "# Example query and documents\n",
        "query = \"What does the Elixir of Umbral Sight do?\"\n",
        "\n",
        "documents = [\n",
        "    \"The Elixir of Umbral Sight grants the drinker flawless vision in all forms of darkness, both magical and mundane, lasting up to one hour. Popular among night-stalkers and shadow scouts.\",\n",
        "    \"Though rare, the Elixir of Umbral Sight can sometimes be found in alchemical markets of the Shrouded Coast, where a single vial may cost up to 300 silver pieces due to demand from adventuring parties.\",\n",
        "    \"The Elixir of Moon's Grace, often confused with Umbral Sight, enhances low-light vision and heightens balance, but does not function in magical darkness.\",\n",
        "    \"An old bard’s tale claims that the Elixir of Umbral Sight reveals not only darkness, but truth — though most scholars dismiss this as poetic exaggeration.\",\n",
        "    \"The Elixir of Umbral Sight is said to grant sight in total darkness, though the exact duration varies. Some claim it fades faster if the user is afraid.\",\n",
        "    \"The Elixir of Wyrm's Breath allows temporary fire-breathing and heat resistance. Its reddish hue and wax-sealed vials are commonly mistaken for Umbral Sight in low lighting.\",\n",
        "    \"Potions brewed from deep mushroom spores often glow faintly, and while they lack any magical effect, they are sometimes used by smugglers for light in underground tunnels.\"\n",
        "]\n",
        "\n",
        "# Rank results\n",
        "results = reranker_model.rank(query, documents, return_documents=True, top_k=4)"
      ],
      "metadata": {
        "id": "iI04uqm2NEBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "OP9K_20TWR8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying mxbai-rerank-base-v2**"
      ],
      "metadata": {
        "id": "Q48m2PD5Wy-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This might require you to restart a session\n",
        "!pip install -q mxbai-rerank"
      ],
      "metadata": {
        "id": "WYSiwHS9NgTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mxbai_rerank import MxbaiRerankV2\n",
        "\n",
        "reranker_model = MxbaiRerankV2(\"mixedbread-ai/mxbai-rerank-base-v2\")\n",
        "\n",
        "# Example query and documents\n",
        "query = \"What does the Elixir of Umbral Sight do?\"\n",
        "\n",
        "documents = [\n",
        "    \"The Elixir of Umbral Sight grants the drinker flawless vision in all forms of darkness, both magical and mundane, lasting up to one hour. Popular among night-stalkers and shadow scouts.\",\n",
        "    \"Though rare, the Elixir of Umbral Sight can sometimes be found in alchemical markets of the Shrouded Coast, where a single vial may cost up to 300 silver pieces due to demand from adventuring parties.\",\n",
        "    \"The Elixir of Moon's Grace, often confused with Umbral Sight, enhances low-light vision and heightens balance, but does not function in magical darkness.\",\n",
        "    \"An old bard’s tale claims that the Elixir of Umbral Sight reveals not only darkness, but truth — though most scholars dismiss this as poetic exaggeration.\",\n",
        "    \"The Elixir of Umbral Sight is said to grant sight in total darkness, though the exact duration varies. Some claim it fades faster if the user is afraid.\",\n",
        "    \"The Elixir of Wyrm's Breath allows temporary fire-breathing and heat resistance. Its reddish hue and wax-sealed vials are commonly mistaken for Umbral Sight in low lighting.\",\n",
        "    \"Potions brewed from deep mushroom spores often glow faintly, and while they lack any magical effect, they are sometimes used by smugglers for light in underground tunnels.\"\n",
        "]\n",
        "\n",
        "# Lets get the scores\n",
        "results = reranker_model.rank(query, documents, return_documents=True, top_k=4)\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "y0qZvP05Nhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "1XQa9rEgOlvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that **v1** and **v2** output scores of different scale.\n",
        "\n",
        "Adding a reranking stage to a RAG pipeline is quite straightforward, and this will be one of your practical tasks in this lesson."
      ],
      "metadata": {
        "id": "JxohJmz-W1jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph databases\n",
        "\n",
        "Vector store are great to encode abstract semantic similarity, but their accuracy in retrieving facts and connections may be mediocre. So, in some cases it's better to check other database types.\n",
        "\n",
        "**Graph databases** typically store **knowledge graphs** which contain\n",
        "\n",
        "* **nodes** which are intities of interest to you - in the example below these are people, organizations, and documents\n",
        "* **relations** between nodes - in the example below these are \"[*person*] works at [*organization*]\", \"[*person*] is spouse of [*person*]\", and \"[*person*] is mentioned in [*document*]\"\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1m9uIJ4Avbr-mrOoZ-tOl0MHql89_TV-H\" width=500 />\n",
        "\n",
        "</center>\n",
        "\n",
        "There is a number of particular implementations of graph databases. We'll try [**Neo4j**](https://neo4j.com/)."
      ],
      "metadata": {
        "id": "WKgYerm_mcFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started with Neo4j\n",
        "\n",
        "First of all, you'll need to registed and get a password. To do this, please go to https://neo4j.com/ and press *Get started for free*. After a simple registration procedure, you'll eventually obtain a **.txt** file with credentials that will likely be formatted as\n",
        "\n",
        "```\n",
        "# Wait 60 seconds before connecting using these details, or login to https://console.neo4j.io to validate the Aura Instance is available\n",
        "NEO4J_URI=<your_uri>\n",
        "NEO4J_USERNAME=<your_username>\n",
        "NEO4J_PASSWORD=<your_password>\n",
        "AURA_INSTANCEID=<your_instance_id>\n",
        "AURA_INSTANCENAME=<your_instance_name>\n",
        "```\n",
        "\n",
        "The following code will, parse such a file, save all the credentials as environmental variables, and initialize connection with your database."
      ],
      "metadata": {
        "id": "x1tzQEf0qiN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai langchain-huggingface langchain-experimental neo4j\n",
        "!pip install -q lancedb pyarrow tiktoken\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "7rUSvCKXrv8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_neo4j_config(file_path='neo4j_credentials.txt'):\n",
        "    \"\"\"\n",
        "    Parse a Neo4j Aura configuration file and extract connection details.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the configuration file\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the parsed connection details\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                # Skip comments and empty lines\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                # Parse key-value pairs\n",
        "                if '=' in line:\n",
        "                    key, value = line.split('=', 1)\n",
        "                    config[key] = value\n",
        "\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "config = parse_neo4j_config()\n",
        "\n",
        "import os\n",
        "\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "\n",
        "os.environ[\"NEO4J_URI\"] = config[\"NEO4J_URI\"]\n",
        "os.environ[\"NEO4J_USERNAME\"] = config[\"NEO4J_USERNAME\"]\n",
        "os.environ[\"NEO4J_PASSWORD\"] = config[\"NEO4J_PASSWORD\"]\n",
        "\n",
        "graph = Neo4jGraph(refresh_schema=False)"
      ],
      "metadata": {
        "id": "zp48eyuXrUcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `graph` is the connection to the Neo4j database.\n",
        "\n",
        "Now, we'll also need our Nebius AI Studio credentials:"
      ],
      "metadata": {
        "id": "LxPTBIVbsMNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"NEBIUS_API_KEY\"] = userdata.get('NEBIUS_API_KEY')"
      ],
      "metadata": {
        "id": "48GjcFB4sp3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring a simple graph database\n",
        "\n",
        "I asked ChatGPT to generate a short story for us to play with:"
      ],
      "metadata": {
        "id": "QXCOhG1Ps-zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"Virenthia is one of the Five Eastern Realms, bordered by Caldrien to the west and Eshkar to the south. Since the War of Withering (1023 AE), it has been ruled by the House of Halveric, a noble dynasty claiming descent from the Moonbound Kings of old.\n",
        "\n",
        "The current monarch of Virenthia, Queen Aeryn Halveric, ascended the throne in 1127 AE following the death of her father, King Thandor Halveric, who ruled for nearly three decades.\n",
        "\n",
        "Virenthia maintains tense diplomatic ties with Eshkar, which has long disputed the control over the border town of Dunmire. In contrast, Virenthia is allied with Caldrien, bound by the marriage pact between Aeryn’s cousin Lord Caelum Halveric and Lady Virelle of Caldrien.\n",
        "\n",
        "Queen Aeryn has named Princess Elira Halveric, her younger sister, as heir presumptive of Virenthia, as she remains unmarried and without children.\"\"\""
      ],
      "metadata": {
        "id": "knSKGMdcvPzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a database capturing\n",
        "\n",
        "* Entities of type `PERSON` and `COUNTRY`\n",
        "* Relationships `RULER_OF`, `NEIGHBOUR_OF` (between countries), and `MENTIONED_IN` (a particular sentence).\n",
        "\n",
        "But how to extract these relations? We could probably do it with rules, but that's not fun! Instead, we'll use LLMs! We can even need to bother with prompts: the **Langchain** library has all the infrastructure; we only need to name the relations and hope that the LLM is powerful enough to understand what we need from it. We'll use **Llama-3.1-70B** served by Nebius AI Studio."
      ],
      "metadata": {
        "id": "33q6S6fjvVPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "# This defines the LLM:\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        "    temperature=0,\n",
        "    model_name=\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
        "\n",
        "llm_transformer = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    allowed_nodes=[\"Character\", \"Country\"],\n",
        "    allowed_relationships=[\n",
        "        (\"Character\",\"CHARACTER_IS_RULER_OF\",\"Country\"),\n",
        "        (\"Character\",\"IS_HEIR_OF\",\"Character\"),\n",
        "        (\"Country\",\"COUNTRY_IS_NEIGHBOUR_TO\",\"Country\")\n",
        "    ],\n",
        "    node_properties=[\"ruler:reign_start_year\"],\n",
        "    strict_mode=True\n",
        ")"
      ],
      "metadata": {
        "id": "T7HGKpAJwK8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the **LLMGraphTransformer** will (hopefully) find all the required relations and properties."
      ],
      "metadata": {
        "id": "38twNCEGzdNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [Document(page_content=sentence) for sentence in story.split(\"\\n\")]\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
        "print(f\"Relationships:{graph_documents[0].relationships}\")"
      ],
      "metadata": {
        "id": "EQjbidASyJky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're ready to populate the actual database, which is accessible through the `graph` endpoint. With the `include_source=True` option toggled, the documents themselves (i.e. the sentences) will also be present in the database with ``"
      ],
      "metadata": {
        "id": "Y_NNhCgv16pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_graph_documents(graph_documents, include_source=True)"
      ],
      "metadata": {
        "id": "jbcFRhK0wJp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now explore your database in the Neo4j visual console: https://console-preview.neo4j.io/tools/explore or try some queries in the \"query\" pane. Just keep in mind that 1-2 minutes may pass before the data is updated on the web side.\n",
        "\n",
        "You'll be able to see something like this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XEXEQk-qMnxbPCxLfRm0KZYYNw98HBuC\" width=800 />\n",
        "\n",
        "</center>\n",
        "\n",
        "The extraction isn't flawless, but still it's a nice start.\n",
        "\n",
        "Then return here to try some queries by API."
      ],
      "metadata": {
        "id": "8gF3vMrk2MyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"MATCH (n) RETURN n LIMIT 10\")"
      ],
      "metadata": {
        "id": "XmKJjiOQ2uMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG with a graph database\n",
        "\n",
        "Now that we have a database describing characters, countries, and relationships between them, we can let an LLM use it to answer questions like \"Who rules Virenthia?\"\n",
        "\n",
        "For that, we're rely on the `GraphCypherQAChain` class from the `langchain_neo4j` library.\n",
        "\n",
        "First of all, we'll refresh the schema to be sure that the `graph` endpoint has up-to-date understanding of the node and connection types."
      ],
      "metadata": {
        "id": "JsOgq0OgOukv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.refresh_schema()"
      ],
      "metadata": {
        "id": "1Ue7nA6fOV1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define the `GraphCypherQAChain` that will wrap all the process for us. Under the hood, three stages will be performed:\n",
        "\n",
        "* The LLM will formulate a query to the graph database using the prompts that Langchain and Neo4j provided\n",
        "* The graph database will run the query, (hopefully) fetching some data\n",
        "* The LLM will formulate the final answer"
      ],
      "metadata": {
        "id": "Q_3ANFsBPqys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_neo4j.chains.graph_qa.cypher import GraphCypherQAChain\n",
        "\n",
        "qa_llm = ChatOpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        "    temperature=0,\n",
        "    model_name=\"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n",
        "\n",
        "chain = GraphCypherQAChain.from_llm(\n",
        "    llm=qa_llm, graph=graph, verbose=True,\n",
        "    allow_dangerous_requests=True # I love this one\n",
        ")"
      ],
      "metadata": {
        "id": "0ybtMbHTK4YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be useful to check that the `chain` has correct understanding of node and relation types. They should be just as in the graph database:"
      ],
      "metadata": {
        "id": "jqzow9fFQaxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.graph_schema)"
      ],
      "metadata": {
        "id": "PmnquCWZOZxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"\"\"\n",
        "Who rules Virenthia?\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "XrmF3IGVNZP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that if you choose an inferior LLM, it may fail to cope with creating the right query. In the example below, Llama-3.1-8B confuses the direction of the `CHARACTER_IS_RULER_OF` relation: instead of\n",
        "\n",
        "```\n",
        "(c:Character)-[:CHARACTER_IS_RULER_OF]->(co:Country {id: \"Virenthia\"})\n",
        "```\n",
        "\n",
        "it suggests\n",
        "\n",
        "```\n",
        "(c:Country {id: \"Virenthia\"})-[:CHARACTER_IS_RULER_OF]->(r:Character)\n",
        "```"
      ],
      "metadata": {
        "id": "OPukCc-mRYn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weak_qa_llm = ChatOpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        "    temperature=0,\n",
        "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "weak_chain = GraphCypherQAChain.from_llm(\n",
        "    llm=weak_qa_llm, graph=graph, verbose=True,\n",
        "    allow_dangerous_requests=True # I love this one\n",
        ")\n",
        "\n",
        "weak_chain.invoke(\"\"\"\n",
        "Who rules Virenthia?\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "q_WrYo9rRg7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearing the traces of our activity from the Neo4j instance\n",
        "\n",
        "Finishing working with the graph database, we'll clear it from out Neo4j instance."
      ],
      "metadata": {
        "id": "gDWmv-z7FM6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"MATCH (n) DETACH DELETE n\")"
      ],
      "metadata": {
        "id": "F1uJv9PO115r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-stage retrieval, take 2: Combining the power of Vector and Graph Databases\n",
        "\n",
        "Vector and graph databases have their own pros and cons:\n",
        "\n",
        "* **Vector stores** are great at establishing *implicit*, topic-level connections between text fragments. For example, vector search draw parallels between discussions of web search and database retrieval.\n",
        "* **Graph databases** capture *explicit* connections between particular entities. For example, it can encapsulate the fact that LanceDB is a vector store.\n",
        "\n",
        "In this, they complement each other perfectly. Thus, adopting both database types in a RAG system may be very beneficial. The two-stage retrieval we'll discuss here is probably the most popular way of combining them.\n",
        "\n",
        "The idea is the following:\n",
        "\n",
        "* On **Stage 1**, a **vector store** is queried, returning several <font color='blue'>documents</font> (<font color='blue'>Doc1</font>, <font color='blue'>Doc2</font>, and <font color='blue'>Doc 3</font> on the image below)\n",
        "* On **Stage 2**, from each of the stage 1 <font color='blue'>documents</font> we make several hops along the **graph database** edges - starting from `MENTIONS` arrows, we get to\n",
        "\n",
        "  - *entities* mentioned in these docs\n",
        "  - and then to other <font color='lightblue'>documents</font> mentioning these *entities*\n",
        "  - and then to yet other *entities* connected to the initial *entities*.\n",
        "\n",
        "  Like in the example below in two hops we reach <font color='lightblue'>Doc4</font>, <font color='green'>Person1</font>, <font color='green'>Person2</font>, <font color='orange'>Org1</font>, and <font color='orange'>Org2</font>.\n",
        "\n",
        "  With Stage 2, we can significantly expand the retrieval's outreach, bringing both new documents and meaningful connections.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1G2B12OMz39uPmYVIprqi4f2SPyyP1z9h\" width=600 />\n",
        "\n",
        "</center>\n",
        "\n",
        "Let's code this!"
      ],
      "metadata": {
        "id": "TAOG5dtRFYnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, we have all the ingredients: both **LanceDB** as a vector store and **Neo4j** as a graph database. A tricky thing will be to ensure that documents have same ids in both LanceDB and Neo4j - so that we cound connect them between stages."
      ],
      "metadata": {
        "id": "NdHpgi-4dde9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll experiment with a slightly larger story.\n",
        "\n",
        "<details>\n",
        "<summary>How we came up with this story</summary>\n",
        "\n",
        "The story isn't entirely fiction. We took info from wiki pages of several rulers of VII century England and Wales - [Penda of Mercia](https://en.wikipedia.org/wiki/Penda_of_Mercia), [Edwin of Nortumbria](https://en.wikipedia.org/wiki/Edwin_of_Northumbria), [Oswald of Northumbria](https://en.wikipedia.org/wiki/Oswald_of_Northumbria), [Eanflæd](https://en.wikipedia.org/wiki/Eanfl%C3%A6d) and several more of their contemporaries. Then we thoroughly renamed the characters and the kingdoms and asked GPT-4.5 to weave everything into a single story. VII century is an exciting period of time, but not very well known, so we hope that LLMs wouldn't see through our disguise.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "2yGTiDSikGKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Nebius-Academy/LLM-Engineering-Essentials/raw/main/topic3/northelm.txt -O northelm.txt"
      ],
      "metadata": {
        "id": "Xh9Sbw9uluGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"northelm.txt\", \"r\") as f:\n",
        "    story = \"\".join(f.readlines())"
      ],
      "metadata": {
        "id": "62SX8hzEmvFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For your convenience we'll repeat the installation code here as well as reload the credentials:"
      ],
      "metadata": {
        "id": "fS65rXPJd_gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain langchain-neo4j langchain-openai langchain-huggingface langchain-experimental neo4j\n",
        "!pip install -q lancedb pyarrow tiktoken\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "uFeu1OXMesWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "from neo4j import GraphDatabase, basic_auth\n",
        "import lancedb\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from lancedb.embeddings import get_registry\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_neo4j import Neo4jGraph"
      ],
      "metadata": {
        "id": "o5qqm_2Xm0Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting your credentials\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"NEBIUS_API_KEY\"] = userdata.get('NEBIUS_API_KEY')\n",
        "\n",
        "def parse_neo4j_config(file_path='neo4j_credentials.txt'):\n",
        "    \"\"\"\n",
        "    Parse a Neo4j Aura configuration file and extract connection details.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the configuration file\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the parsed connection details\n",
        "    \"\"\"\n",
        "    config = {}\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                # Skip comments and empty lines\n",
        "                line = line.strip()\n",
        "                if not line or line.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                # Parse key-value pairs\n",
        "                if '=' in line:\n",
        "                    key, value = line.split('=', 1)\n",
        "                    config[key] = value\n",
        "\n",
        "        return config\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing file: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Change the file name if you need\n",
        "config = parse_neo4j_config(file_path='neo4j_credentials.txt')\n",
        "\n",
        "os.environ[\"NEO4J_URI\"] = config[\"NEO4J_URI\"]\n",
        "os.environ[\"NEO4J_USERNAME\"] = config[\"NEO4J_USERNAME\"]\n",
        "os.environ[\"NEO4J_PASSWORD\"] = config[\"NEO4J_PASSWORD\"]"
      ],
      "metadata": {
        "id": "CqOfxDy2evks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load our story to LanceDB. Note the `chunk_id` field in the `FantasySchema` - they will mirror document ids in the graph database."
      ],
      "metadata": {
        "id": "J6QbRXBnfF_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /tmp/lancedb\n",
        "db = lancedb.connect(\"/tmp/lancedb\")\n",
        "\n",
        "embed_func = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "class FantasySchema(LanceModel):\n",
        "    '''\n",
        "    This is how we store data in the database.\n",
        "    We need to have a vector here, but apart from this, we may have many other fields\n",
        "    '''\n",
        "    text: str = embed_func.SourceField()\n",
        "    vector: Vector(embed_func.ndims()) = embed_func.VectorField(default=None)\n",
        "    chunk_id: str\n",
        "\n",
        "lance_table = db.create_table(\n",
        "    \"fantasy_world\",\n",
        "    mode='overwrite',\n",
        "    schema=FantasySchema\n",
        ")\n",
        "\n",
        "# Split your documents and assign IDs\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=64,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "splitted = []\n",
        "# We have only one story:\n",
        "texts = [story]\n",
        "for text in texts:\n",
        "    docs = text_splitter.create_documents([text])\n",
        "    for doc in docs:\n",
        "        cid = str(uuid.uuid4())\n",
        "        splitted.append({\"text\": doc.page_content, \"chunk_id\": cid})\n",
        "\n",
        "# Ingest into LanceDB\n",
        "lance_table.add(splitted, on_bad_vectors=\"drop\")"
      ],
      "metadata": {
        "id": "Xuz3gdZVm0Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's populate the graph database. Here, we'll store chunk ids in the metadata of each document as the `id` field."
      ],
      "metadata": {
        "id": "P48UfeUIfkAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
        "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "llm_transformer = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    allowed_nodes=[\"Character\", \"Country\"],\n",
        "    allowed_relationships=[\n",
        "        (\"Character\", \"CHARACTER_IS_RULER_OF\", \"Country\"),\n",
        "        (\"Character\", \"IS_HEIR_OF\", \"Character\"),\n",
        "        (\"Country\", \"COUNTRY_IS_NEIGHBOUR_TO\", \"Country\")\n",
        "    ],\n",
        "    strict_mode=True\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=row[\"text\"],\n",
        "        metadata={\n",
        "            \"id\": row[\"chunk_id\"]    # This are the same ids as in the vector store\n",
        "        }\n",
        "    )\n",
        "    for row in splitted\n",
        "]\n",
        "\n",
        "# This may take some time: the LLM will process each document\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
      ],
      "metadata": {
        "id": "kI9s8XSpm0Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to your Neo4j endpoint\n",
        "graph = Neo4jGraph(refresh_schema=False)\n",
        "\n",
        "# Run this query if you want to get rid of previous databases\n",
        "# that may still persist in your Neoj instance\n",
        "graph.query(\"MATCH (n) DETACH DELETE n\")"
      ],
      "metadata": {
        "id": "DhVznmf1w_y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    include_source=True,    # uses metadata['id'] for MERGE\n",
        "    baseEntityLabel=True    # adds __Entity__ for performance\n",
        ")"
      ],
      "metadata": {
        "id": "lW_ESy3lm0iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's double check that our text chunks got the same ids in both LanceDB and Neo4j."
      ],
      "metadata": {
        "id": "jgHngPtBgqjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents[:8]:\n",
        "    print(doc.metadata['id'], doc.page_content)"
      ],
      "metadata": {
        "id": "rvp8_px7gsML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"MATCH (d:Document)\n",
        "RETURN d.id, d.text\n",
        "LIMIT 5\"\"\")"
      ],
      "metadata": {
        "id": "7IYtgLstm0lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a function performing two-stage retrieval:"
      ],
      "metadata": {
        "id": "pafvmpVrhNNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Any, Dict\n",
        "\n",
        "def two_stage_rag(\n",
        "    query: str,\n",
        "    lance_table=None,\n",
        "    graph=None,\n",
        "    lance_schema=None,\n",
        "    max_results: int = 5, # Max documents fetched by vector search\n",
        "    hop_min: int = 1,\n",
        "    hop_max: int = 2,\n",
        "    limit: int = 8, # For hom many documents to seach for paths in the graph\n",
        "    max_paths: int = 8 # Max number of paths to return for each document\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Perform two-stage retrieval using both vector and graph databases.\n",
        "\n",
        "    Args:\n",
        "        query: The user's query\n",
        "        lance_table: LanceDB table\n",
        "        graph: Neo4j graph instance\n",
        "        max_results: Maximum number of vector results to retrieve\n",
        "        hop_min: Minimum number of hops in graph traversal\n",
        "        hop_max: Maximum number of hops in graph traversal\n",
        "        limit: A confusing parameter actually telling\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing query, vector hits, and graph results\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Stage 1: retrieval in LanceDB\n",
        "        docs = lance_table.search(query).limit(max_results).to_pydantic(lance_schema)\n",
        "        chunk_ids = [d.chunk_id for d in docs]  # extract bridge keys\n",
        "\n",
        "        # Stage 2: retrieval in Neo4j\n",
        "        rows = graph.query(\n",
        "            f\"\"\"\n",
        "MATCH (d:Document) WHERE d.id IN $ids\n",
        "OPTIONAL MATCH path=(d)-[*{hop_min}..{hop_max}]-(e)\n",
        "WITH d, COLLECT(path)[0..$maxPaths] AS limitedPaths\n",
        "RETURN\n",
        "  d.id   AS cid,\n",
        "  d.text AS paragraph,\n",
        "  limitedPaths AS paths\n",
        "LIMIT $limit\n",
        "            \"\"\",\n",
        "            params={\"ids\": chunk_ids, \"limit\": limit, \"maxPaths\": max_paths}\n",
        "        )\n",
        "\n",
        "        # Assembling the final output\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"vector_hits\": docs,\n",
        "            \"graph_results\": rows\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in two-stage retrieval: {str(e)}\")\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"vector_hits\": [],\n",
        "            \"graph_results\": []\n",
        "        }\n"
      ],
      "metadata": {
        "id": "lgfkGDqAm0oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = two_stage_rag(\n",
        "    \"Name a heir of a heir of the ruler who unified the kingdoms of Beranhold and Deyrmoor\",\n",
        "    lance_table=lance_table,\n",
        "    graph=graph,\n",
        "    lance_schema=FantasySchema,\n",
        "    max_paths=5)\n",
        "\n",
        "print(\"🔍 Vector Hits:\")\n",
        "for doc in output[\"vector_hits\"]:\n",
        "    print(\"-\", doc.text[:80], \"…\")\n",
        "print(\"\\n🔗 Graph Results:\")\n",
        "for row in output[\"graph_results\"]:\n",
        "    print(f\"Chunk {row['cid']}:\")\n",
        "    for p in row[\"paths\"]:\n",
        "        print(\" \", p)"
      ],
      "metadata": {
        "id": "jBgnvYMXy7Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the graph outputs contain all the paths from Stage 1 docs that Neo4j was able to traverse. To parse and deduplicate this data, we'll have the function `extract_entity_information`. It returns:\n",
        "\n",
        "* Non-document entities (Person, Country etc)\n",
        "* Relationships\n",
        "* Documents with their ids - the latter is needed for further deduplication with documents from LanceDB\n",
        "\n",
        "This data is further deduplicated and merged with documents from Stage 1 by the `process_two_stage_results` functions. The final context is passed to the familiar `answer_with_rag`.\n",
        "\n",
        "Let's try it!"
      ],
      "metadata": {
        "id": "wIv_ML-UhWF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from typing import List, Dict, Any, Union, Optional\n",
        "import uuid\n",
        "\n",
        "# Client setup\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "llama_70b_model = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def extract_entity_information(rows):\n",
        "    \"\"\"\n",
        "    Extract structured information about entities and relationships from graph data.\n",
        "    Also extracts document texts with their IDs for future reference.\n",
        "\n",
        "    Args:\n",
        "        rows: List of row results from graph query, each containing paths\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing:\n",
        "        - entity_info: Formatted string of entity information\n",
        "        - entity_data: Dictionary of structured entity data\n",
        "        - relationships: List of relationship tuples\n",
        "        - documents: Dictionary mapping document IDs to their texts\n",
        "    \"\"\"\n",
        "    if not rows:\n",
        "        return {\n",
        "            \"entity_info\": \"No relevant entity information found.\",\n",
        "            \"entity_data\": {},\n",
        "            \"relationships\": [],\n",
        "            \"documents\": {}\n",
        "        }\n",
        "\n",
        "    # Data structures to collect information\n",
        "    entities = {}       # Store entity properties by ID\n",
        "    relationships = []  # Store relationships as (start_id, rel_type, end_id) tuples\n",
        "    documents = {}      # Store document texts by ID\n",
        "\n",
        "    # Process each row and its paths\n",
        "    for row in rows:\n",
        "        # Store document from the current row if present\n",
        "        if row.get('cid') and row.get('paragraph'):\n",
        "            documents[row['cid']] = row['paragraph']\n",
        "\n",
        "        # Skip if no paths to process\n",
        "        if not row.get('paths'):\n",
        "            continue\n",
        "\n",
        "        # Process each path in the row\n",
        "        for path in row['paths']:\n",
        "            if not isinstance(path, list) or len(path) < 3:\n",
        "                continue  # Skip invalid paths\n",
        "\n",
        "            # First, extract all nodes from the path\n",
        "            for i, item in enumerate(path):\n",
        "                # Process only dictionary items with an ID (nodes)\n",
        "                if isinstance(item, dict) and 'id' in item:\n",
        "                    node_id = item['id']\n",
        "\n",
        "                    # Store document nodes separately\n",
        "                    if 'text' in item and item['text']:\n",
        "                        documents[node_id] = item['text']\n",
        "                    else:\n",
        "                        # Store other entity properties\n",
        "                        # Skip if this entity already exists in our collection\n",
        "                        if node_id not in entities:\n",
        "                            # Extract all properties except 'id'\n",
        "                            props = {k: v for k, v in item.items() if k != 'id' and v and v != 'null'}\n",
        "                            entities[node_id] = props\n",
        "\n",
        "            # Now extract relationships\n",
        "            for i in range(len(path)):\n",
        "                # Look for node -> relationship -> node pattern\n",
        "                if (i < len(path) - 2 and\n",
        "                    isinstance(path[i], dict) and 'id' in path[i] and\n",
        "                    isinstance(path[i+1], str) and\n",
        "                    isinstance(path[i+2], dict) and 'id' in path[i+2]):\n",
        "\n",
        "                    start_id = path[i]['id']\n",
        "                    rel_type = path[i+1]\n",
        "                    end_id = path[i+2]['id']\n",
        "\n",
        "                    # Skip 'MENTIONS' relationships with document nodes\n",
        "                    if rel_type == 'MENTIONS' and (start_id in documents or end_id in documents):\n",
        "                        continue\n",
        "\n",
        "                    # Add relationship if it doesn't already exist\n",
        "                    relationship = (start_id, rel_type, end_id)\n",
        "                    if relationship not in relationships:\n",
        "                        relationships.append(relationship)\n",
        "\n",
        "    # Format the output text\n",
        "    result_lines = []\n",
        "\n",
        "    # Format entities\n",
        "    if entities:\n",
        "        result_lines.append(\"Entities:\")\n",
        "        for entity_id, props in sorted(entities.items()):\n",
        "            prop_str = \", \".join([f\"{k}: {v}\" for k, v in props.items()])\n",
        "            if prop_str:\n",
        "                result_lines.append(f\"  {entity_id}: {prop_str}\")\n",
        "            else:\n",
        "                result_lines.append(f\"  {entity_id}\")\n",
        "\n",
        "    # Format relationships\n",
        "    if relationships:\n",
        "        if result_lines:  # Add a separator if we already have content\n",
        "            result_lines.append(\"\")\n",
        "        result_lines.append(\"Relationships:\")\n",
        "        for start, rel, end in sorted(relationships):\n",
        "            result_lines.append(f\"  {start} --[{rel}]--> {end}\")\n",
        "\n",
        "    # Create the final formatted string\n",
        "    entity_info = \"\\n\".join(result_lines) if result_lines else \"No relevant entity information found.\"\n",
        "\n",
        "    return {\n",
        "        \"entity_info\": entity_info,\n",
        "        \"entity_data\": entities,\n",
        "        \"relationships\": relationships,\n",
        "        \"documents\": documents\n",
        "    }\n",
        "\n",
        "# Example modification to process_two_stage_results to use the improved function\n",
        "def process_two_stage_results(results):\n",
        "    \"\"\"\n",
        "    Process the results from two-stage retrieval to create a context for the LLM.\n",
        "\n",
        "    Args:\n",
        "        results: Output from two_stage_rag function\n",
        "\n",
        "    Returns:\n",
        "        Formatted context string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not results:\n",
        "            return \"No relevant information found.\"\n",
        "\n",
        "        # Process vector hits\n",
        "        vector_paragraphs = [doc.text for doc in results.get(\"vector_hits\", [])]\n",
        "\n",
        "        # Process graph results - using the improved extract_entity_information\n",
        "        extracted_info = extract_entity_information(results.get(\"graph_results\", []))\n",
        "\n",
        "        # Get the document texts\n",
        "        graph_paragraphs = list(extracted_info[\"documents\"].values())\n",
        "\n",
        "        # Remove duplicates with vector results\n",
        "        graph_paragraphs = [p for p in graph_paragraphs if p not in vector_paragraphs]\n",
        "\n",
        "        # Combine all contexts\n",
        "        context_parts = []\n",
        "\n",
        "        all_paragraphs = vector_paragraphs + graph_paragraphs\n",
        "        if all_paragraphs:\n",
        "            context_parts.append(\"Document Paragraphs:\\n\" + \"\\n\\n\".join(all_paragraphs))\n",
        "\n",
        "        if extracted_info[\"entity_info\"] and extracted_info[\"entity_info\"] != \"No relevant entity information found.\":\n",
        "            context_parts.append(\"Graph Information:\\n\" + extracted_info[\"entity_info\"])\n",
        "\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing two-stage results: {str(e)}\")\n",
        "        return \"Error processing retrieval results.\"\n",
        "\n",
        "def answer_with_rag(\n",
        "    prompt: str,\n",
        "    system_prompt: Optional[str] = None,\n",
        "    max_tokens: int = 512,\n",
        "    client: OpenAI = nebius_client,\n",
        "    retrieval_model: str = llama_70b_model,\n",
        "    generation_model: str = llama_8b_model,\n",
        "    lance_table = None,\n",
        "    lance_schema=FantasySchema,\n",
        "    graph = None,\n",
        "    prettify: bool = True,\n",
        "    temperature: float = 0.6,\n",
        "    max_results: int = 5,\n",
        "    hop_min: int = 1,\n",
        "    hop_max: int = 2,\n",
        "    max_paths: int = 10,\n",
        "    verbose: bool = False\n",
        ") -> Union[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Generate an answer using enhanced RAG with two-stage retrieval (vector + graph).\n",
        "\n",
        "    Args:\n",
        "        prompt: User's question or prompt\n",
        "        system_prompt: Instructions for the LLM\n",
        "        max_tokens: Maximum number of tokens in the response\n",
        "        client: OpenAI client instance\n",
        "        retrieval_model: Model used for retrieval (if using LLM-based retrieval)\n",
        "        generation_model: Model used for answer generation\n",
        "        lance_table: LanceDB table for vector search\n",
        "        graph: Neo4j graph instance\n",
        "        prettify: Whether to format the output text\n",
        "        temperature: Temperature for response generation\n",
        "        max_results: Maximum number of vector results to retrieve\n",
        "        hop_min: Minimum number of hops in graph traversal\n",
        "        hop_max: Maximum number of hops in graph traversal\n",
        "        verbose: Whether to return the search results as well\n",
        "\n",
        "    Returns:\n",
        "        Generated response incorporating retrieval results or dict with answer and context\n",
        "    \"\"\"\n",
        "    context = \"No relevant information found.\"\n",
        "\n",
        "    # Perform two-stage retrieval if both databases are available\n",
        "    if lance_table and graph:\n",
        "        try:\n",
        "            retrieval_results = two_stage_rag(\n",
        "                query=prompt,\n",
        "                lance_table=lance_table,\n",
        "                lance_schema=lance_schema,\n",
        "                graph=graph,\n",
        "                max_results=max_results,\n",
        "                hop_min=hop_min,\n",
        "                hop_max=hop_max\n",
        "            )\n",
        "            context = process_two_stage_results(retrieval_results)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during retrieval: {str(e)}\")\n",
        "            context = f\"Retrieval failed. Proceeding with limited context.\"\n",
        "    # Fallback to just vector search if graph is not available\n",
        "    elif lance_table:\n",
        "        try:\n",
        "            search_results = lance_table.search(prompt).limit(max_results).to_pydantic(lance_schema)\n",
        "            context = \"\\n\\n\".join([record.text for record in search_results])\n",
        "        except Exception as e:\n",
        "            print(f\"Error during vector search: {str(e)}\")\n",
        "            context = f\"Vector search failed. Proceeding with limited context.\"\n",
        "\n",
        "    # Construct messages with retrieved context\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "\n",
        "    # Add user prompt with context\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Answer the following query using the context provided.\n",
        "\n",
        "        <context>\n",
        "        {context}\n",
        "        </context>\n",
        "\n",
        "        <query>{prompt}</query>\n",
        "        \"\"\"\n",
        "    })\n",
        "\n",
        "    # Generate completion\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=generation_model,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        if prettify:\n",
        "            answer = prettify_string(completion.choices[0].message.content)\n",
        "        else:\n",
        "            answer = completion.choices[0].message.content\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"context\": context\n",
        "            }\n",
        "        else:\n",
        "            return answer\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error generating answer: {str(e)}\"\n",
        "        print(error_message)\n",
        "\n",
        "        if verbose:\n",
        "            return {\n",
        "                \"answer\": error_message,\n",
        "                \"context\": context\n",
        "            }\n",
        "        else:\n",
        "            return error_message\n"
      ],
      "metadata": {
        "id": "NNYz_ht3y7U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = answer_with_rag(\"Name a heir of a heir of the ruler who unified the kingdoms of Beranhold and Deyrmoor\",\n",
        "                generation_model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                lance_table=lance_table, graph=graph, verbose=True)\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "id": "Xw4s_JyYMdRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though I'd prefer an explanation, this seems to be accurate. Aldric was a heir of Eanfrith, who was a son of Aelfric, who was indeed the ruler who unified Beranhold and Deyrmoor.\n",
        "\n",
        "At the same time, if we switch off graph retrieval, we'll get an incorrect answer:"
      ],
      "metadata": {
        "id": "z8Tqeqdlz6z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = answer_with_rag(\"Name a heir of a heir of the ruler who unified the kingdoms of Beranhold and Deyrmoor\",\n",
        "                generation_model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
        "                lance_table=lance_table, graph=None, verbose=True)\n",
        "result[\"answer\"]"
      ],
      "metadata": {
        "id": "0cSnY3qy9sr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We haven't implemented graph-only search here, but feel free to do it using the code from the previous section."
      ],
      "metadata": {
        "id": "jdQ7cvj91Yy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final notes: getting the correct graph\n",
        "\n",
        "Graph retrieval gives a powerful boost to vector search, helping with brining accurate connections - in which vector search may oftern fail. However, its accuracy shouldn't be taken for granted. LLMs may misinterpret the nature of connections while constructing the graph, and if you look closely you'll see some examples of that in our story-based database.\n",
        "\n",
        "Thus, it's better to use a more powerful LLM at the graph construction phase. Yes, it will increase cost, but a malformed knowledge graph would likely cost you more. Also, if you can parse some connections manually, don't hesitate to do it. And of course, a bit of prompt engineering won't hurt. Let's briefly discuss how to do it.\n",
        "\n",
        "Generally, the `LLMTransformer` is a chain. Its first link has the following prompt template:"
      ],
      "metadata": {
        "id": "QhiYnbP41g0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_transformer.chain.first.messages"
      ],
      "metadata": {
        "id": "bUveWG0X9YP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, the system prompt is quite long and detailed. You can study it and add some additional considerations about your entities and relationships. Also, few-shot examples won't hurt!\n",
        "\n",
        "That's how it can be done:"
      ],
      "metadata": {
        "id": "28JdPV9J9a_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = llm_transformer.chain.first.messages[0].prompt.template + \"\"\"\n",
        "<Some additional considerations>\n",
        "<Examples>\n",
        "\"\"\"\n",
        "\n",
        "human_prompt = \"\"\" <Your comments go here>\n",
        "Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: {input}\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", human_prompt)\n",
        "])\n",
        "\n",
        "llm_transformer1 = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    allowed_nodes=[\"Character\", \"Country\"],\n",
        "    allowed_relationships=[\n",
        "        (\"Character\",\"CHARACTER_IS_RULER_OF\",\"Country\"),\n",
        "        (\"Character\",\"IS_HEIR_OF\",\"Character\"),\n",
        "        (\"Country\",\"COUNTRY_IS_NEIGHBOUR_TO\",\"Country\")\n",
        "    ],\n",
        "    strict_mode=True,\n",
        "    prompt=prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "hgvbZBkM8_wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: Adding a Reranker\n"
      ],
      "metadata": {
        "id": "ztjX8BwcVD-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this task, you'll need to add the **reranking stage** to the `answer_with_rag` function.\n",
        "\n",
        "Compare the results with and without reranking and with different reranking models. Try to come up with tricky and confusing prompts."
      ],
      "metadata": {
        "id": "3YX7so4cX-SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lancedb pyarrow tiktoken -q\n",
        "!pip install -qU langchain-text-splitters"
      ],
      "metadata": {
        "id": "2-eylR7ediPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\n",
        "        \"\\n\\n\",\n",
        "        \"\\n\",\n",
        "        \".\",\n",
        "        \" \"\n",
        "    ],\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=128,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "HSQtLO_cdiPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "from functools import partial\n",
        "\n",
        "import lancedb\n",
        "from lancedb.pydantic import LanceModel, Vector\n",
        "from lancedb.embeddings import get_registry\n",
        "\n",
        "import openai\n",
        "import pyarrow as pa"
      ],
      "metadata": {
        "id": "Ux-p70dSdiPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from markdown import markdown\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def markdown_to_text(markdown_string):\n",
        "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
        "\n",
        "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
        "    html = markdown(markdown_string)\n",
        "\n",
        "    html = re.sub(r'<!--((.|\\n)*)-->', '', html)\n",
        "    html = re.sub('<code>bash', '<code>', html)\n",
        "\n",
        "    # extract text\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    text = ''.join(soup.findAll(text=True))\n",
        "\n",
        "    text = re.sub('```(py|diff|python)', '', text)\n",
        "    text = re.sub('```\\n', '\\n', text)\n",
        "    text = re.sub('-         .*', '', text)\n",
        "    text = text.replace('...', '')\n",
        "    text = re.sub('\\n(\\n)+', '\\n\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_files(input_dir=\"transformers/docs/source/en/\", output_dir=\"docs\"):\n",
        "    # Convert string paths to Path objects\n",
        "    input_dir = Path(input_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "\n",
        "    # Check if input directory exists\n",
        "    assert input_dir.is_dir(), \"Input directory doesn't exist\"\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for root, subdirs, files in tqdm(os.walk(input_dir)):\n",
        "        root_path = Path(root)\n",
        "        for file_name in files:\n",
        "            file_path = root_path / file_name\n",
        "            parent = root_path.stem if root_path.stem != input_dir.stem else \"\"\n",
        "\n",
        "            if file_path.is_file():\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    md = f.read()\n",
        "                text = markdown_to_text(md)\n",
        "\n",
        "                output_file = output_dir / f\"{parent}_{Path(file_name).stem}.txt\"\n",
        "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text)\n"
      ],
      "metadata": {
        "id": "_i5MUrTsew8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "hiVA36IEew8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_files()"
      ],
      "metadata": {
        "id": "-U8qSrqtew8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line is needed in case you've ran this cell before to clear the db dir\n",
        "!rm -rf /tmp/lancedb\n",
        "\n",
        "db = lancedb.connect(\"/tmp/lancedb\")\n",
        "\n",
        "# We use this model as the encoder: https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "embed_func = get_registry().get(\"huggingface\").create(name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "class BasicSchema(LanceModel):\n",
        "    '''\n",
        "    This is how we store data in the database.\n",
        "    We need to have a vector here, but apart from this, we may have many other fields\n",
        "    '''\n",
        "    text: str = embed_func.SourceField()\n",
        "    vector: Vector(embed_func.ndims()) = embed_func.VectorField(default=None)\n",
        "\n",
        "lance_table = db.create_table(\n",
        "    \"transformer_docs\",\n",
        "    mode='overwrite',\n",
        "    schema=BasicSchema\n",
        ")\n",
        "\n",
        "# Populating the database\n",
        "\n",
        "from tqdm import tqdm\n",
        "splitted_docs = []\n",
        "\n",
        "for file in tqdm(os.listdir(\"docs\")):\n",
        "    with open(\"docs/\"+file, \"r\") as f:\n",
        "        text = f.read()\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        splitted_docs.extend([{\"text\": doc.page_content} for doc in docs])\n",
        "\n",
        "lance_table.add(\n",
        "    splitted_docs,\n",
        "    on_bad_vectors='drop'  # or 'fill' with fill_value=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "02GseFXJdnBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "Here is the function you need to modify:"
      ],
      "metadata": {
        "id": "vh61n5GOAJP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from math import e\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "reranker_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def search_table(table, query, max_results=15):\n",
        "    return table.search(query).limit(max_results).to_pydantic(BasicSchema)\n",
        "\n",
        "def search_result_to_context(search_result):\n",
        "    return \"\\n\\n\".join(\n",
        "        [record.text for record in search_result]\n",
        "    )\n",
        "\n",
        "def search_results_to_text(search_result):\n",
        "    return [record.text for record in search_result]\n",
        "\n",
        "def answer_with_rag(\n",
        "    prompt: str,\n",
        "    system_prompt=None,\n",
        "    max_tokens=512,\n",
        "    client=nebius_client,\n",
        "    model=llama_8b_model,\n",
        "    table=None,\n",
        "    prettify=True,\n",
        "    temperature=0.6,\n",
        "    max_results=5,\n",
        "    verbose=False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer using RAG (Retrieval-Augmented Generation) with database search.\n",
        "\n",
        "    Args:\n",
        "        prompt: User's question or prompt\n",
        "        system_prompt: Instructions for the LLM\n",
        "        max_tokens: Maximum number of tokens in the response\n",
        "        client: OpenAI client instance\n",
        "        model: Model identifier\n",
        "        search_client: Search client instance (for example, Tavily)\n",
        "        prettify: Whether to format the output text\n",
        "        temperature: Temperature for response generation\n",
        "        search_depth: Depth of web search ('basic' or 'advanced')\n",
        "        verbose: whether to return the search results as well\n",
        "\n",
        "    Returns:\n",
        "        Generated response incorporating search results\n",
        "    \"\"\"\n",
        "    # Perform database search\n",
        "    max_stage_1_results = 10\n",
        "    if table:\n",
        "        try:\n",
        "            stage_1_results = search_table(table, prompt,\n",
        "                                           max_results=max_stage_1_results)\n",
        "            stage_1_results = search_results_to_text(stage_1_results)\n",
        "            ranked_results = reranker_model.rank(\n",
        "                prompt,\n",
        "                stage_1_results,\n",
        "                return_documents=True,\n",
        "                top_k=max_results\n",
        "            )\n",
        "\n",
        "            print(\"<<<<<< Reranked search results >>>>>\")\n",
        "            for result in ranked_results:\n",
        "                print(result)\n",
        "\n",
        "            search_results = [i[\"text\"] for i in ranked_results if i.get(\"text\")]\n",
        "\n",
        "        except (AttributeError, ValueError) as err:\n",
        "            print(err)\n",
        "            search_results = []\n",
        "    else:\n",
        "        search_results = []\n",
        "\n",
        "    # Construct messages with search results\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        })\n",
        "\n",
        "    context = \"\\n\\n\".join(search_results)\n",
        "    # Add user prompt\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\n",
        "            f\"\"\"Answer the following query using the context provided.\n",
        "\n",
        "            <context>\\n{context}\\n</context>\n",
        "\n",
        "            <query>{prompt}</query>\n",
        "            \"\"\"\n",
        "    })\n",
        "\n",
        "    # Generate completion\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        answer = prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        answer = completion.choices[0].message.content\n",
        "\n",
        "    if verbose:\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"search_results\": search_results\n",
        "        }\n",
        "    else:\n",
        "        return answer"
      ],
      "metadata": {
        "id": "xrUTpK--g3CB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "results = answer_with_rag(\"\"\"How to quantize a model in 4 bits?\"\"\",\n",
        "               client=client,\n",
        "                model=model,\n",
        "               table=lance_table, verbose=True, max_results=4)\n",
        "print(results[\"answer\"])"
      ],
      "metadata": {
        "id": "99sH-wUAg3GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the retrieved context pieces and their reranker scores:"
      ],
      "metadata": {
        "id": "OuinAg_FCOXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ZYMFn5Lrg3In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set up a particular reranker and run the whole pipeline on our favourite query."
      ],
      "metadata": {
        "id": "slqeygZdCVJT"
      }
    }
  ]
}