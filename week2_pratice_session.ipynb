{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nemanovich/LLM-essentials/blob/main/week2_pratice_session.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this week's practice session we'll learn:\n",
        "\n",
        "- How to use LangChain, one of the most popular library to simplify LLM interaction;\n",
        "- How to add plugins to an LLM with LangChain;\n",
        "- How to interact with a database using an LLM."
      ],
      "metadata": {
        "id": "VUADqJGLqxKm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCb0dbvH6AQ0"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain is a handy library which supplies a whole infrastucture around LLMs (both open source and available by API) allowing to quickly establish LLM-powered services. It can help you with many LLM related tasks, from prompt optimisation to creating multi-call LLM agents.\n",
        "\n",
        "Let's see how to use LangChain. First of all, download the library:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = open(\".open-ai-api-key\")\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ['KAGGLE_KEY'] = userdata.get(\"KAGGLE_KEY\")"
      ],
      "metadata": {
        "id": "SGGArYZZ32aO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5dx0eA456AQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279451c7-d3c5-469f-a96e-a8edf77c7042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain langchain_openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest thing you can do with LangChain is just calling an LLM.  We'll do it for OpenAI API:\n",
        "\n",
        "Note: The base model for OpenAI class is `text-davinci-003`, the significance of that will become apparent later"
      ],
      "metadata": {
        "id": "54DvPP6bHeRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xspBOmsx6AQ1"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1etZ63Oi6AQ1",
        "outputId": "bd93795d-3a69-452a-8726-fdbbf5b7a046",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " behavior and anatomy.\n",
            "\n",
            "Behavior: Cats are generally considered to be more independent and aloof than dogs. They are known for their graceful and solitary nature, often spending hours grooming themselves and napping. Cats also have a tendency to be more territorial and may not get along well with other cats. On the other hand, dogs are social animals and thrive on companionship. They are known for their loyalty and love to be around their owners. Dogs are also more easily trainable and can perform a variety of tasks, while cats are typically less easily trained and may have a harder time learning new behaviors.\n",
            "\n",
            "Anatomy: Cats and dogs have different physical characteristics that set them apart. Cats have retractable claws, while dogs' claws are always exposed. This allows cats to climb and jump with precision and agility, while dogs are better adapted for running and digging. Cats also have a more flexible spine, allowing them to squeeze into tight spaces and land on their feet when falling. Dogs, on the other hand, have a stronger sense of smell and hearing, making them excellent hunters and guard dogs. They also have a higher body temperature and sweat through their tongue, while cats sweat through their paws. Overall, cats and dogs have evolved differently to suit their respective lifestyles and behaviors.\n"
          ]
        }
      ],
      "source": [
        "print(llm.invoke(\n",
        "    \"What is the difference between cats and dogs? In two words:\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsUuk0l56AQ1"
      },
      "source": [
        "As you can see, the interface is already much simpler, compared to writing it on your own.\n",
        "\n",
        "LangChain also distinguishes between LLM's and Chat models.\n",
        "\n",
        "A difference is very subtle and mostly affect the format in which you pass data. LLM's are a pure text completion models, which means they input text and output text. Where is ChatModels work on a list of ChatMessages, which can be AIMessage, HumanMessage or SystemMessage (this difference we covered in week 1) and return an AIMessage.\n",
        "\n",
        "Newer OpenAI only implement chat interface, for example gpt-3.5-turbo, gpt-4, etc. This means, that you cannot use them as an LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7mnscJPR6AQ1",
        "outputId": "4282fe60-7c00-4a89-9568-ce59e158d2aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Behavior, loyalty', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 19, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C5chKsqzLSagXSnjogD7CBXvaZeol', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--45d0d24c-3820-4983-9fc4-7de79fc9e33c-0', usage_metadata={'input_tokens': 19, 'output_tokens': 3, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(name='gpt-4o-mini')\n",
        "chat.invoke([\n",
        "    HumanMessage(content=\"In two words what's the difference \"\\\n",
        "        \"between Cats and Dogs?\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Draw your attention to the fact that we received a `AIMessage` instead of a string"
      ],
      "metadata": {
        "id": "D3pUViKnaDlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basics"
      ],
      "metadata": {
        "id": "xmrvEGUREoHO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQsTqfV6AQ2"
      },
      "source": [
        "#### Prompt templates\n",
        "\n",
        "A useful feature of LangChain is Prompt templates.\n",
        "\n",
        "If you need to use the same prompt structure with different parameters, prompt templates can save you from the text duplication. See, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "w8rAkutU6AQ2",
        "outputId": "1950a44d-4999-45fb-a11e-150ba6fc3185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the national cousine of Australia?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"What is the national cousine of {country}?\"\n",
        ")\n",
        "prompt.format(country=\"Australia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPUcVsiP6AQ2"
      },
      "source": [
        "Now our imaginary user needs only to select a country instead of creating a whole prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chaining\n",
        "\n",
        "One of the main pillars of LangChain is the concept of chaining, that is of combining several LLM calls, external function calls, etc.\n",
        "\n",
        "Much like you combine layers in neural networks, but here we have a much more diverse set of tools.\n",
        "\n",
        "A very basic chain consists of prompt template and an LLM call. It's almost like a \"function\" for an LLM:"
      ],
      "metadata": {
        "id": "xT5bRZBfYfk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xnP3cxER6AQ2",
        "outputId": "1d815992-3e98-464f-b203-672498c50bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe national cuisine of Australia is often described as \"modern Australian\" or \"Aussie cuisine,\" which is a fusion of different cultural influences, including British, Indigenous Australian, European, and Asian. Some popular dishes in Australia include meat pies, fish and chips, BBQ meats, lamingtons, and pavlova. However, due to its diverse population, there is no specific national dish or cuisine that represents all of Australia.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "chain.invoke(\"Australia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`StrOutputParser` here transforms output of our LLM, which in this case is in `messages`, in the format of a string. In case you'd ask for multiple output options, this parser give you the most likely one.\n",
        "\n",
        "Note by the way that, although we had a typo in the prompt template (\"cousine\" instead of \"cuisine\"), LLM managed to mitigate with it. You probably shouldn't rely on this too much, but generally LLMs, that are trained on data containing typos as well, can be able to cope with some amount of mistakes in the prompts."
      ],
      "metadata": {
        "id": "9S8kWoDEcZy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_model_response = llm.invoke(\"Hello, do you like cats?\")\n",
        "print(text_model_response)\n",
        "print(f\"Type: {type(text_model_response)}\")\n",
        "\n",
        "chat_response = chat.invoke(\"Hello, do you like cats?\")\n",
        "print(chat_response)\n",
        "print(f\"Type: {type(chat_response)}\")\n",
        "\n",
        "\n",
        "parsed_text_model_output = output_parser.invoke(text_model_response)\n",
        "print(parsed_text_model_output)\n",
        "print(type(parsed_text_model_output))\n",
        "\n",
        "parsed_chat_output = output_parser.invoke(chat_response)\n",
        "print(parsed_chat_output)\n",
        "print(type(parsed_chat_output))"
      ],
      "metadata": {
        "id": "QCS9lCgKVR-t",
        "outputId": "8af3be5b-2543-42db-b80f-b470324e5403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "As an AI, I do not have personal preferences or emotions. I am programmed to assist and communicate with humans.\n",
            "Type: <class 'str'>\n",
            "content=\"Hello! As an AI, I don't have personal preferences, but I can provide information or answer any questions you may have about cats.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 14, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C5chRcMMNJKjcgtxfighF1JbDb5MQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--95cc0c1b-e845-4ff0-84f8-033287cb471f-0' usage_metadata={'input_tokens': 14, 'output_tokens': 28, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "\n",
            "\n",
            "As an AI, I do not have personal preferences or emotions. I am programmed to assist and communicate with humans.\n",
            "<class 'str'>\n",
            "Hello! As an AI, I don't have personal preferences, but I can provide information or answer any questions you may have about cats.\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sequential chain\n",
        "\n",
        "We can combine multiple calls in a simple sequential chain, where the output of one call become the input of another call."
      ],
      "metadata": {
        "id": "DPe71TMvacPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_prompt = PromptTemplate.from_template(\n",
        "    \"What is the capital of {country}?\"\n",
        ")\n",
        "first_chain = first_prompt | llm | output_parser\n",
        "\n",
        "second_prompt = PromptTemplate.from_template(\n",
        "    \"{city} is the capital of which country?\"\n",
        ")\n",
        "second_chain = second_prompt | llm | output_parser\n",
        "\n",
        "simple_sequential_chain = first_chain | second_chain"
      ],
      "metadata": {
        "id": "Cx4X1mCkaXY5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuitively now we should receive the same thing we inputted, let's try."
      ],
      "metadata": {
        "id": "7pmR19cKb3je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_sequential_chain.invoke(\"United Kingdom\")"
      ],
      "metadata": {
        "id": "5Yv8wkEZb_Uj",
        "outputId": "c6118c55-03d4-4057-9892-603f437c0eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nUnited Kingdom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to make a more complicated chain, where outputs fill in specific variables, we'll have to use an `itemgetter`."
      ],
      "metadata": {
        "id": "IAj6rAHzdWod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "first_prompt = PromptTemplate.from_template(\n",
        "    \"Name a city of {country} starting with {letter}\",\n",
        ")\n",
        "first_chain = first_prompt | llm | output_parser\n",
        "\n",
        "second_prompt = PromptTemplate.from_template(\n",
        "    \"What is the main attraction in {city}?\"\n",
        ")\n",
        "second_chain = second_prompt | llm | output_parser\n",
        "\n",
        "sequential_chain = {\n",
        "    \"country\": itemgetter(\"country\"),\n",
        "    \"letter\": itemgetter(\"letter\"),\n",
        "    \"city\": first_chain\n",
        "} | second_chain | output_parser"
      ],
      "metadata": {
        "id": "3zWcl6-ad23j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In that case you'll have to pass input arguments as a dict."
      ],
      "metadata": {
        "id": "r7ZVAjufe6Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_chain.invoke({\"country\": \"France\", \"letter\": \"P\"})"
      ],
      "metadata": {
        "id": "6zKb6TXRe7B-",
        "outputId": "fd7b022c-882b-4352-bc6e-b5c2d22b270c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe main attraction in Paris is the Eiffel Tower.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Debugging\n",
        "\n",
        "As you can see we only get the output of the last chain. But what if we want to see what happened in the first one?"
      ],
      "metadata": {
        "id": "Q3dPu7-nINLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n"
      ],
      "metadata": {
        "id": "PaHMNUsaIL1D"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_chain.invoke(\n",
        "    {\"country\": \"France\", \"letter\": \"P\"},\n",
        "    config={'callbacks': [ConsoleCallbackHandler()]}\n",
        ")"
      ],
      "metadata": {
        "id": "VN-9TkUHIMal",
        "outputId": "f30f0de8-6a04-42e8-8647-5c94e10f8a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableLambda] [2ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"France\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"P\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Name a city of France starting with P\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > llm:OpenAI] [322ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\n\\nParis\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 2,\n",
            "      \"total_tokens\": 10,\n",
            "      \"prompt_tokens\": 8\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\\n\\nParis\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\n\\nParis\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city> > chain:RunnableSequence] [325ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\n\\nParis\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<country,letter,city>] [332ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\",\n",
            "  \"city\": \"\\n\\nParis\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"country\": \"France\",\n",
            "  \"letter\": \"P\",\n",
            "  \"city\": \"\\n\\nParis\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"What is the main attraction in \\n\\nParis?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:OpenAI] [982ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 13,\n",
            "      \"total_tokens\": 22,\n",
            "      \"prompt_tokens\": 9\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.32s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\n\\nThe main attraction in Paris is the Eiffel Tower.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe main attraction in Paris is the Eiffel Tower.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b086ZL_6AQ2"
      },
      "source": [
        "### Task 1\n",
        "\n",
        "In this task we'll learn how to rewrite ChatGPT interaction code to LangChain.\n",
        "\n",
        "In the previous week we inplemented translate and summarise function. Rewrite it using `SequentialChain`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "llm = OpenAI()\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "summarise_prompt = PromptTemplate(\n",
        "    input_variables=['text'],\n",
        "    template=\"Write a short summary of the following text.\\n{text}\"\n",
        ")\n",
        "summarise_chain = summarise_prompt | llm | output_parser\n",
        "\n",
        "translate_prompt = PromptTemplate(\n",
        "    input_variables=['summary', 'target_language'],\n",
        "    template=\"Translate the following text to {target_language}:\\n{summary}\"\n",
        ")\n",
        "translate_chain = translate_prompt | llm | output_parser\n",
        "\n",
        "summarise_and_translate_chain = {\n",
        "    \"text\": itemgetter(\"text\"),\n",
        "    \"summary\": summarise_chain,\n",
        "    \"target_language\": itemgetter(\"target_language\")\n",
        "} | translate_chain | output_parser"
      ],
      "metadata": {
        "id": "VAJ_9prRcqCT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = open(\"wikipedia_article_japanese.txt\").read()\n",
        "\n",
        "summarise_and_translate_chain.invoke(\n",
        "    {'text': article, \"target_language\": \"English\"}\n",
        ")"
      ],
      "metadata": {
        "id": "KlveDg_YgCdX",
        "outputId": "a3cf47ec-e2a1-4c8d-d7b9-4f4cedd86891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'によっても異なる。\\n\\n\\nIn addition, products featuring paw pads are also commonly seen in adult goods.\\n\\nPaw pads are also used as trademarks for hanko (name stamps) and stamps (refer to Nekkiu).\\n\\nPaw pads are the raised and hairless part of the bottom of the feet of animals in the order Carnivora, and are officially called metatarsal pads. The paw pads have sections such as the palmar pad, digital pads, carpal pads, plantar pads, and toe pads, and they mainly serve to cushion the impact during walking. They can be found in animals such as cats, dogs, bears, weasels, rodents, and marsupials. The shape and softness of paw pads vary among individuals and can also differ depending on the environment they inhabit. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-iv_kRr6AQ3"
      },
      "source": [
        "## LangChain Agents and Memory\n",
        "\n",
        "In this part we'll explore two cool features of LangChain: **Agents** and **Memory**. You will learn how to:\n",
        "\n",
        "- access internet inside a chain;\n",
        "- remember the conversation history and adjust to it.\n",
        "\n",
        "**Agents** allow you to use tooling like web search, calling apis, math, python code etc. (they are known as \"Plugins\" in ChatBPT Web UI) to achive the goal of the given task.\n",
        "\n",
        "**Memory** allows you to keep a state of the conversation, just like what you see in the WebUI of ChatGPT.\n",
        "\n",
        "If you combine the two you can essentially get the same interface as ChatGPT WebUI has with plugins."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web search\n",
        "\n",
        "The are plenty of search engines available. We'll try DuckDuckGo, but feel free to use any other for your projects.\n",
        "\n",
        "Let's install the library."
      ],
      "metadata": {
        "id": "DWi_byUVn4EQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YvnlZU7i6AQ3",
        "outputId": "383ce84d-1834-4717-d6b8-89be6012b3ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install duckduckgo_search langchain_community -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A search engine is a **tool**. Which is essentially a function with specific signature, that our LLM can use."
      ],
      "metadata": {
        "id": "F6XX-TydoMRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
        "\n",
        "results_tool = DuckDuckGoSearchResults()\n",
        "display(results_tool(\"What is the name of the cat from Shrek\"))\n",
        "\n",
        "search_tool = DuckDuckGoSearchRun()\n",
        "display(search_tool(\"What is the name of the cat from Shrek\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "nGlitpdqF6Ib",
        "outputId": "d87afee1-658c-4f6d-9582-3b3d0c989953"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"snippet: Oct 13, 2009 · I'm looking for a command line tool which gets an IP address and returns the host name, for Windows., title: windows - Resolve host name from IP address - Server Fault, link: https://serverfault.com/questions/74042/resolve-host-name-from-ip-address, snippet: This is a Canonical Question about Active Directory domain naming. After experimenting with Windows domains and domain controllers in a virtual environment, I've realized that having an …, title: Windows Active Directory naming best practices? - Server Fault, link: https://serverfault.com/questions/76715/windows-active-directory-naming-best-practices, snippet: Mar 26, 2023 · What could be the possible problems with accessing a Windows file server shares using a DNS CNAME instead of the actual computer name? The file server is joined to an Active …, title: Accessing Windows file server by alias name, link: https://serverfault.com/questions/1127178/accessing-windows-file-server-by-alias-name, snippet: I occasionally get the following 421 error: Misdirected Request The client needs a new connection for this request as the requested host name does not match the Server Name Indication (SNI..., title: ssl - 421 Misdirected Request - Server Fault, link: https://serverfault.com/questions/916724/421-misdirected-request\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"Oct 13, 2009 · I'm looking for a command line tool which gets an IP address and returns the host name, for Windows. This is a Canonical Question about Active Directory domain naming. After experimenting with Windows domains and domain controllers in a virtual environment, I've realized that having an … Mar 26, 2023 · What could be the possible problems with accessing a Windows file server shares using a DNS CNAME instead of the actual computer name? The file server is joined to an Active … I occasionally get the following 421 error: Misdirected Request The client needs a new connection for this request as the requested host name does not match the Server Name Indication (SNI... Oct 25, 2023 · This is a new installation of Server 2022 Standard 21H2. I'm trying to configure the SMTP Server so that a client application can send emails internally. When I open IIS 6.0 …\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an agent, which uses this tool is pretty simple"
      ],
      "metadata": {
        "id": "LOg9Qw6BF4kY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NbAIf4UF6AQ3",
        "outputId": "bcbe71b9-97c7-40a3-fd2a-57e3495e2135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3673428984.py:6: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(name='gpt-4o-mini')\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[search_tool], llm=llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see it in action"
      ],
      "metadata": {
        "id": "Fhz-spICIdMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the name of the cat from Shrek\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjlqzsirIXQP",
        "outputId": "591a148b-e093-4fb6-8245-4d16f02d91d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should search for the name of the cat from Shrek\n",
            "Action: duckduckgo_search\n",
            "Action Input: cat from Shrek name\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe cat <<EOF syntax is very useful when working with multi-line text in Bash, eg. when assigning multi-line string to a shell variable, file or a pipe. Examples of cat <<EOF syntax usage in Bash: cat \"Some text here.\" > myfile.txt Possible? Such that the contents of myfile.txt would now be overwritten to: Some text here. This doesn't work for me, but also doesn't throw any errors. … The original order is in fact backwards. Certs should be followed by the issuing cert until the last cert is issued by a known root per IETF's RFC 5246 Section 7.4.2 This is a sequence (chain) of … May 14, 2009 · 46 There are a few ways to pass the list of files returned by the find command to the cat command, though technically not all use piping, and none actually pipe directly to cat. The … Oct 23, 2018 · The problem is that echo removes the newlines from the string. How do you append to a file a string which contains newlines?\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThis search did not provide the answer to the name of the cat from Shrek\n",
            "Action: duckduckgo_search\n",
            "Action Input: Puss in Boots Shrek name\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPus is an exudate, typically white-yellow, yellow, or yellow-brown, formed at the site of inflammation during infections, regardless of … The meaning of PUSS is cat. Jun 14, 2023 · Pus is a thick fluid containing dead tissue, cells, and bacteria. Your body often produces it when it’s fighting off an infection, … Nov 16, 2023 · Pus is a whitish-yellow, yellow, green, or brown-yellow protein-rich fluid called liquor puris that accumulates at the site of an … Jan 26, 2023 · Pus is a fluid that contains a mixture of dead skin cells, white blood cells, and infectious material. The body produces pus as …\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mSearching for \"Puss in Boots Shrek name\" gave me more relevant information\n",
            "Final Answer: The name of the cat from Shrek is Puss in Boots.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the name of the cat from Shrek',\n",
              " 'output': 'The name of the cat from Shrek is Puss in Boots.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, agent not only chose to perform a web search, but also read the results and gave you the final answer.\n",
        "\n",
        "You can read more about how ReAct agents work [here](https://react-lm.github.io/)"
      ],
      "metadata": {
        "id": "gm3f2p3Ro-kP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FgQVMGK6AQ3"
      },
      "source": [
        "### Memory\n",
        "\n",
        "Memory allows an agent to memorize the previous interaction with the user and act according to it. Let's try to add memory and make a small conversation.\n",
        "\n",
        "We'll use the simplest construct called `ConversationBufferMemory` but you can actually use more complicated ones, which save conversation history to a database for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "onLblITM6AQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f0c603-4c3c-43e1-f439-d088b932bf00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3171711846.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content=\"Hello, ChatGPT! How's your day?\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"I'm doing well, thanks for asking!\", additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"Hello, ChatGPT! How's your day?\")\n",
        "memory.chat_memory.add_ai_message(\"I'm doing well, thanks for asking!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- We used `memory_key` = 'chat_history', which is why memory returns us messages under that key\n",
        "- We used `return_messages` = True, which is why memory returns messages to us instead of concatenated strings."
      ],
      "metadata": {
        "id": "6sHjyOQvKPrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat history is explicitly present in the prompt as the `history` variable.\n",
        "\n",
        "Now, let's define the chain:\n"
      ],
      "metadata": {
        "id": "iMXcj7sRp97q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With LangChain you can initialise an agent with memory still in just a couple lines.\n",
        "You need to make sure to use an appropriate agent type (in this case the \"CHAT_CONVERSATION\" ReAct agent.\n",
        "\n",
        "Note: Admittedly the documentation for this is a bit chaotic, so you'll have to play a bit before you get a good result."
      ],
      "metadata": {
        "id": "Jn33rMjoKh5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "llm = ChatOpenAI(name='gpt-4o-mini')\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=[search_tool],\n",
        "    memory=memory,\n",
        "    llm=llm,\n",
        "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION ,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "LArn-hMIKnpD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's to observe some memorization happening!"
      ],
      "metadata": {
        "id": "9AUQNjiZKvoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"What is the name of the cat from Shrek?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1H-v7_MKzBC",
        "outputId": "198885e5-b735-4212-edc5-391e1d33a458"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The name of the cat from Shrek is Puss in Boots.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the name of the cat from Shrek?',\n",
              " 'chat_history': [HumanMessage(content='What is the name of the cat from Shrek?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The name of the cat from Shrek is Puss in Boots.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'The name of the cat from Shrek is Puss in Boots.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"How many sequels were there in this film?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMadDgVsK6Tc",
        "outputId": "6e487583-f628-4445-eef1-f943ab84f78c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"duckduckgo_search\",\n",
            "    \"action_input\": \"Shrek film sequels\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mShrek is an anti-social ogre who loves the solitude of his swamp and enjoys fending off mobs and intruders. One day, his life is interrupted after he inadvertently saves a talkative Donkey from … May 18, 2001 · Shrek: Directed by Andrew Adamson, Vicky Jenson. With Mike Myers, Eddie Murphy, Cameron Diaz, John Lithgow. A mean lord exiles fairytale creatures to the swamp of … On a mission to retrieve a princess from a fire-breathing dragon, gruff ogre Shrek teams up with an unlikely compatriot — a wisecracking donkey. You may be looking for Shrek (character) or Shrek (franchise). Shrek is a 2001 American computer-animated fantasy comedy film produced and distributed by DreamWorks Pictures. It … Shrek (Mike Myers) goes on a quest to rescue the feisty Princess Fiona (Cameron Diaz) with the help of his loveable Donkey (Eddie Murphy) and win back the deed to his swamp from …\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"There are four sequels in the Shrek film series.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'How many sequels were there in this film?',\n",
              " 'chat_history': [HumanMessage(content='What is the name of the cat from Shrek?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='The name of the cat from Shrek is Puss in Boots.', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='How many sequels were there in this film?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='There are four sequels in the Shrek film series.', additional_kwargs={}, response_metadata={})],\n",
              " 'output': 'There are four sequels in the Shrek film series.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector stores"
      ],
      "metadata": {
        "id": "al5Njtut6hgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"/content/langchain_vectorstore.png\", width=600)"
      ],
      "metadata": {
        "id": "0L3fkDRC6O3R",
        "outputId": "772e8431-3b80-4c51-8f71-29559d8efbf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: '/content/langchain_vectorstore.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/content/langchain_vectorstore.png'"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: '/content/langchain_vectorstore.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: '/content/langchain_vectorstore.png'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the goals of this week is to create your own RAG-based app. **RAG** (**R**etrieval **A**ugmented **G**eneration) is a concept of supporting a generative model with some kind of a retrieval tool which allows to get more faithful results and less hallucinations. This is crucial when we need to supply our users with facts, for example, if we're creating a navigation tool for a company's internal wiki.\n",
        "\n",
        "Actually, we already touched upon RAG when we used DuckDuckGo. This time we'll retrieve data from a specific type of database - **vector store**.\n",
        "\n",
        "The idea behind vector storages is to represent data items as **embeddings** (real vectors). When we receive a search query, we also somehow make it into an embedding and look for its nearest neighbors in the vector space which can be done rather quickly if somewhat approximately. If your embedding model produces vectors with strong semantic information embedded into it, you can have very high quality retrieval.\n",
        "\n",
        "Vector storages emerged long before transformers, but, but because transformer models offer exceptional text understanding capabilities, using them to construct embeddings for vector storage systems is very popular. A typical AI-powered vector database query tool works like that:\n",
        "\n",
        "- An LLM reformulates user's prompt into a vector store query;\n",
        "- An embedding model is used to map the query into the database vector space;\n",
        "- Vector store returns several items whose embeddings are nearest neighbors of the query's embedding;\n",
        "- An LLM is used to process search results into a nice human readable output.\n",
        "\n",
        "In this practice session you'll getting acquainted with vector databases, and in the homework you'll assemble all the pipeline using LangChain.\n"
      ],
      "metadata": {
        "id": "B-kZXHDXxJFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are quite a few vector stores available. We will employ the system called [Faiss](https://github.com/facebookresearch/faiss). It is a state-of-the-art library made by Meta for creating vector databases, which is used by a lot of production solutions.\n",
        "\n",
        "We will use an IELTS essay dataset as a source of long texts, we want to search through."
      ],
      "metadata": {
        "id": "gR-55q0DBY1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please make sure to put your credentials in an appropriate location following the instruction here https://github.com/Kaggle/kaggle-api#api-credentials"
      ],
      "metadata": {
        "id": "aVtCbFzmhe5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle faiss-cpu tiktoken -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MRe2lUhg7gI",
        "outputId": "8bcc43bf-d2cf-46f9-ba8b-d1fcfb0097ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export KAGGLE_CONFIG_DIR=\"/content/\" && kaggle datasets download mazlumi/ielts-writing-scored-essays-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjMhdBS9hBTN",
        "outputId": "57501348-f38a-4ed1-c751-353dd530d446"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mazlumi/ielts-writing-scored-essays-dataset\n",
            "License(s): other\n",
            "Downloading ielts-writing-scored-essays-dataset.zip to /content\n",
            "  0% 0.00/674k [00:00<?, ?B/s]\n",
            "100% 674k/674k [00:00<00:00, 595MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ielts-writing-scored-essays-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SnJW885iRan",
        "outputId": "d5b09dd6-984e-4ea9-c001-87022eceb0f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ielts-writing-scored-essays-dataset.zip\n",
            "  inflating: ielts_writing_dataset.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the data:"
      ],
      "metadata": {
        "id": "YHlDfhDaB22Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "metadata": {
        "id": "lGGip06Wnybv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pandas.options.display.max_colwidth = 100\n",
        "reviews = pandas.read_csv(\"ielts_writing_dataset.csv\")\n",
        "reviews.head(2).dropna(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "H8ZiLTYS2oFt",
        "outputId": "d78b991f-525b-4775-9833-30972b3ffa2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Task_Type  \\\n",
              "0          1   \n",
              "1          2   \n",
              "\n",
              "                                                                                              Question  \\\n",
              "0  The bar chart below describes some changes about the percentage of people were born in Australia...   \n",
              "1  Rich countries often give money to poorer countries, but it does not solve poverty. Therefore, d...   \n",
              "\n",
              "                                                                                                 Essay  \\\n",
              "0  Between 1995 and 2010, a study was conducted representing the percentages of people born in Aust...   \n",
              "1  Poverty represents a worldwide crisis. It is the ugliest epidemic in a region, which could infec...   \n",
              "\n",
              "   Overall  \n",
              "0      5.5  \n",
              "1      6.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dae7073-a766-4a59-bdf1-5845a175cbd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Task_Type</th>\n",
              "      <th>Question</th>\n",
              "      <th>Essay</th>\n",
              "      <th>Overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The bar chart below describes some changes about the percentage of people were born in Australia...</td>\n",
              "      <td>Between 1995 and 2010, a study was conducted representing the percentages of people born in Aust...</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Rich countries often give money to poorer countries, but it does not solve poverty. Therefore, d...</td>\n",
              "      <td>Poverty represents a worldwide crisis. It is the ugliest epidemic in a region, which could infec...</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dae7073-a766-4a59-bdf1-5845a175cbd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3dae7073-a766-4a59-bdf1-5845a175cbd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3dae7073-a766-4a59-bdf1-5845a175cbd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e7a9c262-37ff-4794-a5ad-b1adac58b78d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e7a9c262-37ff-4794-a5ad-b1adac58b78d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e7a9c262-37ff-4794-a5ad-b1adac58b78d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "reviews",
              "summary": "{\n  \"name\": \"reviews\",\n  \"rows\": 1435,\n  \"fields\": [\n    {\n      \"column\": \"Task_Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 402,\n        \"samples\": [\n          \"Providing a national system in a country where the unemployed receive a regular payment only encourages people not to seek work and puts an unreasonable strain on a country\\u2019s financial resources.Discuss this statement and give your opinion.Give reasons for your answer and include any relevant examples from your knowledge or experience.You should write at least 250 words.\",\n          \"The fact that enormous sums are paid for pieces of art is not acceptable at a time when many people around the world live in poverty. Discuss this statement and give your opinion.Give reasons for your answer and include any relevant examples from your knowledge or experience.You should write at least 250 words.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Essay\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1274,\n        \"samples\": [\n          \"There are two possible ways to interprete the significance of history experience for our everyday life.\\nThe first point is to ignore the history experience. The main reason is based on the opinion that history never repeats. All historical persons that lived at the elder times never made their decisions in the conditions those surround us. The world has changed significantly. Technology, information, transport, medicine, human rights - all these important parts of our life has changed since the elder times. So, we can not directly copy the decisions of different problems of the elder times to our present life.\\nThe second point is to use the history experience in our present life. This opinion is based on observed samples of very similar intervals of history. At different time periods in different countries there were some events or historical processes being very similar. There are a lot of examples illustrating this thought. All countries of Medieval Europe and Asia past through the feodalism period. Then leaders of that counties decided that the feodalism is not the most effective way to control the country and annuled it. Nowadays we know that experience and, I believe, there will be no attempts to buld the feodalism again. The second example is the practice of conquestions. There were a lot of great conquerrors in the world's history: Alexander Macedonian, Caesar, Chinggis-khan, Napoleon, Gitler. All the listed persons conquerred big territories, their armies were very strong. But some time past and their empires were broken. Sometimes their enemies united against the conquerrors, sometimes they just died and their successors could not save the empires. But always the empires died. Nowadays we have no any signs of old borders of that empires. Actually all state borders are based on nations' borders, not on the old conquestions.\\nI think the second opinion is better. The reason is following. There are a lot of institutions in our life that we like. For example, human rights, property rights, education, medicine. All these institutions appeared as a result of historical experince. If our predecessors ignored that experience, we would never live in the modern society. In that case we would have no modern medicine and education, also we would live in the society with slaver. So, I think that we should thoroughly study the historical experience and use its conclusions in our present life.\",\n          \"The bar chart illustrates the number of minutes of telephone calls in Australia, made to various localities or devices, from 2001 to 2008.\\nOver the course of the eight years during which the data was sampled, it can be noted that the number of phone call minutes made to mobile phones has increased by the greatest proportion. In 2001, 2 billion minutes worth of phone call time was recorded. By 2008, this number had increased exponentially to 46 billion minutes.\\nThe bar chart illustrates that between the years 2001 and 2005, the number of phone calls made to local numbers gradually increased. However, from 2006, these calls decreased steadily until 2008, with a number that is only slightly higher than national and international calling minutes.\\nThe number of phone call minutes made to national as well as international locations consistently increased in the allocated time frame.\\nIt is also clear that local telephone calls account for the majority of phone calls made in Australia, and telephone calls made to mobile phones account for the least.\\nIn summary, the number of telephone call minutes made in Australia in different categories, varied during the years 2001 to 2008.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Examiner_Commen\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 62,\n        \"samples\": [\n          \"This essay covers the task. It has a good structure, however the conclusion paragraph is too big \\u2013 consider splitting it into 2 paragraphs, with the last one being the conclusion. As to the structure of sentences, there are several sentences that should be rephrased (see comments for suggestions). The grammar also needs some attention (see underlined in blue comments for details). Overall, this looks like a band 6.5 essay.\",\n          \"This essay is too short, 190 words instead of the minimum requirement of 250. It doesn\\u2019t say anything about girls being influenced by their mothers, which is also a part of the task \\u2013 therefore the task is only partially covered. The sentences are not complex enough, there are grammatical mistakes and inaccuracies (see comments underlined in blue). Overall, this looks like a Band 5.5 essay\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Task_Response\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coherence_Cohesion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lexical_Resource\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Range_Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0582370857889938,\n        \"min\": 1.0,\n        \"max\": 9.0,\n        \"num_unique_values\": 14,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text splitters\n",
        "\n",
        "The length of the documents that we could store in a vector storage is limited by the context length of your models. The texts we work with are often longer, so we need **Text Splitters** to cut the texts into pieces.\n",
        "\n",
        "First of all, let's check out how big our documents are:"
      ],
      "metadata": {
        "id": "3kwgiluki-jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no truncation of text\n",
        "pandas.options.display.max_colwidth = 100_000_000"
      ],
      "metadata": {
        "id": "wbXyMhrJqx_D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import re\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "tkMFYx88jg9X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows_as_single_string = reviews.apply(\n",
        "    lambda row: (re.sub(' +', ' ', row.to_string().replace(\"\\n\", \" \"))),\n",
        "    axis=1\n",
        ")\n",
        "max(map(lambda text: len(enc.encode(text)), rows_as_single_string))"
      ],
      "metadata": {
        "id": "fKB2-2kojQIL",
        "outputId": "8f078e36-3bb4-4801-eedf-843aa542878f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "772"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though this is less then 4096 max ChatGPT tokens, models typically don't undrestand long texts well enough, so it's better to split this item."
      ],
      "metadata": {
        "id": "tD5DesJhtTnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a document list for our database"
      ],
      "metadata": {
        "id": "7lVTnn9T29ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = rows_as_single_string.tolist()"
      ],
      "metadata": {
        "id": "xfFIKnJOusJU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a simple splitter called `CharacterTextSplitter`. It splits text on `separator` then gathers chunks based on `chunk size` as measured by a `length_function`. `chunk_overlap` controlls how much of the previous chunk we want to include in the next one for continuity.\n",
        "\n",
        "Let's see an example."
      ],
      "metadata": {
        "id": "Uj1ulK1txH0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\" \",\n",
        "    chunk_size=32,\n",
        "    chunk_overlap=4,\n",
        "    length_function=lambda text: len(enc.encode(text)),\n",
        ")"
      ],
      "metadata": {
        "id": "USA1UNLMxhq2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "texts = text_splitter.create_documents(documents)\n",
        "display(texts[0])\n",
        "display(texts[1])\n",
        "display(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "tUdFxZ5Pxpj-",
        "outputId": "8f35fcb8-9492-4ba1-f28e-f37d943ecc58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 108, which is longer than the specified 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='Task_Type 1 Question The bar chart below describes some changes about the percentage of people')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='of people were born in Australia and who were born outside Australia living in urban,')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content='in urban, rural and town between 1995 and 2010.Summarise the information by')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RecursiveCharacterTextSplitter` is very similar to `CharacterTextSplitter`, except for the splitting and gathering logic. It inputs a list of `separators` (the default is [\"\\n\\n\", \"\\n\", \" \", \"\"]), which it then used in the same order as in the list. That means that first we split paragraphs, then if they are bigger than `chunk_size` we split on sentences, and so on. This helps the chunks to be a bit more cohesive."
      ],
      "metadata": {
        "id": "wbPbrYHbwkql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=32,\n",
        "    chunk_overlap=4,\n",
        "    length_function=lambda text: len(enc.encode(text)),\n",
        "    add_start_index=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0Hxas7VStTGg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.create_documents(documents)\n",
        "display(texts[0])\n",
        "display(texts[1])\n",
        "display(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "eO3jYEO7uJ-z",
        "outputId": "2272cb74-4bc2-4b25-8141-ed087116041c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': 0}, page_content='Task_Type 1 Question The bar chart below describes some changes about the percentage of people were born in Australia and who were born outside Australia living in urban, rural')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': -1}, page_content='in urban, rural and town between 1995 and 2010.Summarise the information by selecting and reporting the main features and make comparisons where relevant.')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': 288}, page_content='comparisons where relevant. Essay Between 1995 and 2010, a study was conducted representing the percentages of people born in Australia, versus people born outside')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably the most reasonable way to split is not by characters but by tokens using the model's tokenizer. LangChain supports creating a text splitter directly from tiktoken."
      ],
      "metadata": {
        "id": "LiQwGHMlyZZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=32,\n",
        "    chunk_overlap=4,\n",
        "    add_start_index=True\n",
        ")"
      ],
      "metadata": {
        "id": "bbmEZFUyx9KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.create_documents(documents)\n",
        "display(texts[0])\n",
        "display(texts[1])\n",
        "display(texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "ze5hzd1ZyS0r",
        "outputId": "4645d2d7-ee51-4ca3-bdf2-1c4110b6f8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': 0}, page_content='Task_Type 1 Question The bar chart below describes some changes about the percentage of people were born in Australia and who were born outside Australia living in urban, rural')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': -1}, page_content='in urban, rural and town between 1995 and 2010.Summarise the information by selecting and reporting the main features and make comparisons where relevant. Essay Between')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Document(metadata={'start_index': 316}, page_content='Essay Between 1995 and 2010, a study was conducted representing the percentages of people born in Australia, versus people born outside Australia, living in urban, rural,')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector database creation\n",
        "\n",
        "Let's create a database of segments of IELTS essays and examinator comments."
      ],
      "metadata": {
        "id": "seeLrywj3gq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=256,\n",
        "    chunk_overlap=16,\n",
        "    add_start_index=True\n",
        ")\n",
        "splitted_documents = text_splitter.create_documents(documents)\n",
        "db = FAISS.from_documents(splitted_documents, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "jdvev55b39hT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can perform similarity search using our embeddings"
      ],
      "metadata": {
        "id": "z5XatzjE4ovm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"An awesome essay about bar charts\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "FeErhxd_4F_n",
        "outputId": "898b2d66-8ec0-4e44-f595-3b724a9ae5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task_Type 1 Question The bar charts below shows the number of visits to a community website in the first and second year of use.Summarize the information by selecting and reporting the main features and mae comparisons with relevant. Essay The bar chart illustrates the quantity of visits by the thousands paid to a community website within the first two years of use.\\\\nOverall, there is a greater upward trend from the second year of use compared to the first year of use. In addition to that, in both years the website undergoes a drastic fluctuation in numbers. It can be observed that initially in the month of September, number of visits in the first year of use are lower than second year of use, but numbers of the former subsequently surpasses the latter in the final month of August.\\\\nIn regards to the first year of use, quantity of visits increases from about 2000 visits in September to 10000 visits within 2 months and remains constant for another month. Following that, numbers plummet from December to February, reaching a low-point of less than approximately 500 visits. The numbers rise gradually from February onwards until it reaches it's highest at 15000 by August.\\\\nOn the other hand, during the second year of use, the number of visits rises sharply\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"A poorly written essay\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nb0o2tO_5gcf",
        "outputId": "4054f89b-9b25-4cd5-d0f6-224fe5704e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'but you must offer more arguments regarding why you agree or disagree. There are many spelling, punctuation and article errors. The essay is easy to follow but has the appearance of the writer running short of time. Task_Response NaN Coherence_Cohesion NaN Lexical_Resource NaN Range_Accuracy NaN Overall 5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific OpenAI api capabilities\n",
        "\n",
        "Since the creation of LangChain, OpenAI's api actually added a lot of creature comforts on it's own, so some of the funcitonality is being duplicated a bit now.\n"
      ],
      "metadata": {
        "id": "yQQquP4vWfrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "99stviAwWjTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = open(\".open-ai-api-key\")\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "non_json_output = client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description'}],\n",
        "    model=\"gpt-4o-mini\",\n",
        ").choices[0].message.content\n",
        "print(non_json_output)\n",
        "\n",
        "json_output = client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "print(json_output)"
      ],
      "metadata": {
        "id": "ryrB2UfDXitX",
        "outputId": "8efad8b1-5418-425a-f514-18f5993d90f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Character Name:** Lirael Thorne\n",
            "\n",
            "**Class:** Shadow Mage\n",
            "\n",
            "**Description:** Lirael Thorne is a mysterious figure shrouded in the whispers of the night. With long, flowing silver hair that seems to absorb the surrounding light and piercing violet eyes, she exudes an aura of both elegance and danger. As a Shadow Mage, Lirael has mastered the art of manipulating darkness to conceal her presence and create illusions that can confuse and terrify her foes. Her attire consists of a fitted, midnight-blue cloak adorned with glowing runes, offering her stealth and protection.\n",
            "\n",
            "Lirael hails from a hidden enclave, where she was trained in the ancient customs of shadow magic. Torn between her desire for power and a profound sense of responsibility, she often grapples with the ethical implications of her abilities. Despite her enigmatic demeanor, she is fiercely loyal to those she trusts, using her skills to help her allies and protect the innocent from the dangers lurking in the shadows. With a penchant for riddles and a quick wit, Lirael often leaves her companions guessing about her next move, both in and out of combat.\n",
            "{\n",
            "  \"character\": {\n",
            "    \"name\": \"Elysia Darkweaver\",\n",
            "    \"class\": \"Shadow Sorceress\",\n",
            "    \"description\": \"Elysia is a master of dark magic, drawing power from the shadows to manipulate the minds of her enemies and bend them to her will. With her flowing black robes that seem to absorb light, and her piercing violet eyes, she excels in deception and intrigue. Elysia carries an ancient grimoire that holds secrets of forgotten spells and curses, making her both a formidable foe and a valuable ally in the darkest of times.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "1T5XjuVIZJqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "HOFWiZMFZNz_",
        "outputId": "5e0f8d4a-9751-4394-c7a2-d829c572fd45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'character': {'name': 'Elysia Darkweaver',\n",
              "  'class': 'Shadow Sorceress',\n",
              "  'description': 'Elysia is a master of dark magic, drawing power from the shadows to manipulate the minds of her enemies and bend them to her will. With her flowing black robes that seem to absorb light, and her piercing violet eyes, she excels in deception and intrigue. Elysia carries an ancient grimoire that holds secrets of forgotten spells and curses, making her both a formidable foe and a valuable ally in the darkest of times.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model for our outputs:"
      ],
      "metadata": {
        "id": "eB0ImQP5ZjzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    response_format=CharacterProfile,\n",
        ")\n",
        "\n",
        "completion.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "r_jEZ2wiZpD_",
        "outputId": "33974cdb-b41d-42b1-cd2a-e9647e7e0856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Elara Nightshade', age=27, special_skills=['Archery', 'Potion Brewing', 'Stealth Navigation'], traits=['Loyal', 'Adaptable', 'Cunning'], character_class='Ranger', origin='Elderwood Forest')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "FfXwmnGeacJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAPI Tool Usage\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "nGZM-mwVNVxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo_search -q"
      ],
      "metadata": {
        "id": "RDVewymxaiFD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "search = DDGS()\n",
        "search.text(keywords=\"What is the capital of France\", max_results=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHy65SEdOA4A",
        "outputId": "b8d2bc02-770b-456e-8dd6-93c755f2c294"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1589957108.py:3: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  search = DDGS()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Capital of France Crossword Clue - NYT Crossword Answers',\n",
              "  'href': 'https://nytcrosswordanswers.org/capital-of-france-crossword-clue/',\n",
              "  'body': 'May 6, 2020 answer of Capital Of France clue in NYT Crossword Puzzle. There is One Answer total, Euros is the most recent and it has 5 letters.'},\n",
              " {'title': \"Capital of France's Côte d'Or Crossword Clue\",\n",
              "  'href': 'https://nytcrosswordanswers.org/capital-of-frances-cote-dor-crossword-clue/',\n",
              "  'body': 'March 18, 2019 answer of Capital Of Frances Cote Dor clue in NYT Crossword Puzzle. There is One Answer total, Dijon is the most recent and it has 5 letters.'},\n",
              " {'title': 'Tour de France stage Crossword Clue - NYT Crossword Answers',\n",
              "  'href': 'https://nytcrosswordanswers.org/tour-de-france-stage-crossword-clue/',\n",
              "  'body': 'March 28, 2025 answer of Tour De France Stage clue in NYT Crossword Puzzle. There are Two Answers total, Etape is the most recent and it has 5 letters.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for OpenAI's client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `keywords` parameter.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameter are for.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "wqz5uYsDa4l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search-text\",\n",
        "            \"description\": \"Retrieves results from DuckDuckGo web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"keywords\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"keywords\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=\"gpt-4o-mini\"\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfkpK-EqNthN",
        "outputId": "e2363e31-68bd-4bfb-b0b4-38e74443c050"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-C5cnDgvRKZHXtoHzvT1Ep3XE41uY4', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_ga4iMurTLWx3HwPCrKDTwqDi', function=Function(arguments='{\"keywords\":\"cat from Shrek name\"}', name='search-text'), type='function')]))], created=1755456795, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=18, prompt_tokens=91, total_tokens=109, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "TDoc1RE-bpUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9WJEQd2QhI0",
        "outputId": "2b9bf692-3a56-418a-98c2-f09855954b00"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessageFunctionToolCall(id='call_ga4iMurTLWx3HwPCrKDTwqDi', function=Function(arguments='{\"keywords\":\"cat from Shrek name\"}', name='search-text'), type='function')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now based on this functionality, we can create a function to answer using web search."
      ],
      "metadata": {
        "id": "BrdyPXNUQIzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def chat_completion_with_web_search(query):\n",
        "    ready_to_answer = False\n",
        "    messages = []\n",
        "    messages.append({\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"If you are asked about the factual information, \"\\\n",
        "        \"create a search function call instead of answering directly.\"\\\n",
        "        \"If you already searched, use the results to give an answer.\"})\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "    while not ready_to_answer:\n",
        "        chat_response = client.chat.completions.create(\n",
        "            messages=messages, tools=tools, model=\"gpt-4o-mini\"\n",
        "        ).choices[0].message\n",
        "        messages.append(chat_response.to_dict())\n",
        "        if chat_response.tool_calls:\n",
        "            if chat_response.tool_calls[0].function.name == \"search-text\":\n",
        "                print(\"Searching the web\")\n",
        "                call_arguments = json.loads(\n",
        "                    chat_response.tool_calls[0].function.arguments\n",
        "                )\n",
        "                print(f\"Call arguments: {call_arguments}\")\n",
        "                web_results = str(search.text(**call_arguments))\n",
        "                print(f\"Results: {web_results}\")\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": web_results,\n",
        "                    \"tool_call_id\": chat_response.tool_calls[0].id\n",
        "                })\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported tool {chat_response.tool_calls[0].function.name}\")\n",
        "        else:\n",
        "            print(\"Answering the question\")\n",
        "            messages.append({\"role\": \"assistant\", \"content\": chat_response.content})\n",
        "            ready_to_answer = True\n",
        "    return messages[-1]['content']"
      ],
      "metadata": {
        "id": "NHJ1dh4TQFwd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion_with_web_search(\"How many episodes in Star Wars?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "QzJCAwm7Qg-v",
        "outputId": "2c265bd8-f8e6-43a9-be4d-41fe330ea2f1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching the web\n",
            "Call arguments: {'keywords': 'how many episodes in Star Wars series'}\n",
            "Results: [{'title': 'How to Watch Every Star Wars Movie and Series in Order - IGN', 'href': 'https://www.ign.com/articles/star-wars-movies-tv-shows-chronological-order', 'body': ''}, {'title': 'Star Wars Movies and Series Viewing Guide | StarWars.com', 'href': 'https://www.starwars.com/news/star-wars-movies-and-series-guide', 'body': 'May 4, 2025 · Check out the two lists below — release order and chronological order — of every Star Wars movie and series, including live-action and animation, to help you on your Star …'}, {'title': 'Every Star Wars Movie and TV Show, in Release Order', 'href': 'https://www.hollywoodreporter.com/lists/star-wars-movies-tv-shows-release-order/', 'body': ''}, {'title': 'Star Wars - Movies & TV Series Chronological Order', 'href': 'https://www.imdb.com/list/ls072034866/', 'body': 'As the Clone Wars sweep the galaxy, Anakin Skywalker and his new Padawan, Ahsoka Tano, embark on a mission to rescue the kidnapped son of Jabba the Hutt. However, the renegade …'}]\n",
            "Searching the web\n",
            "Call arguments: {'keywords': 'total number of Star Wars movies and episodes'}\n",
            "Results: [{'title': 'Complete List of STAR WARS Movies - IMDb', 'href': 'https://www.imdb.com/list/ls029559286/', 'body': 'As the Clone Wars sweep the galaxy, Anakin Skywalker and his new Padawan, Ahsoka Tano, embark on a mission to rescue the kidnapped son of Jabba the Hutt. However, the renegade …'}, {'title': 'Star Wars movies in order: Chronological and release | Space', 'href': 'https://www.space.com/star-wars-movies-in-order', 'body': ''}, {'title': 'How many Star Wars movies are there? Full list of the films in order', 'href': 'https://www.usatoday.com/story/entertainment/movies/2022/08/08/how-many-star-wars-movies/10207734002/', 'body': ''}, {'title': 'Star Wars Movies and Series Viewing Guide | StarWars.com', 'href': 'https://www.starwars.com/news/star-wars-movies-and-series-guide', 'body': 'May 4, 2025 · Check out the two lists below — release order and chronological order — of every Star Wars movie and series, including live-action and animation, to help you on your Star …'}, {'title': 'How Many Star Wars Movies Are There? The Definitive List', 'href': 'https://darkskiesfilm.com/how-many-star-war-movies-are-there/', 'body': 'Jul 5, 2025 · There are currently 12 theatrically released Star Wars movies in the main canon, including the original trilogy, the prequel trilogy, the sequel trilogy, and standalone anthology …'}]\n",
            "Answering the question\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The exact total number of episodes and movies in the Star Wars franchise can vary. Currently, there are 12 theatrically released Star Wars movies in the main canon. However, if you're also interested in the TV series and animated components, that number will increase substantially.\\n\\nFor a more detailed breakdown, you can check the following resources:\\n- [Star Wars Movies and Series Viewing Guide](https://www.starwars.com/news/star-wars-movies-and-series-guide)\\n- [Complete List of STAR WARS Movies - IMDb](https://www.imdb.com/list/ls029559286/) \\n\\nIf you have specific series or films in mind, please let me know!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latency\n",
        "\n",
        "Depending on the model (size of the model), provider and some specific parameters, the latency of completion calls can vary a lot.\n",
        "\n",
        "Let's write a small function to measure latency and test it on OpenAI's and Anthropic's models."
      ],
      "metadata": {
        "id": "T62MyF8F26PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai anthropic -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-T8m7PN753Ki",
        "outputId": "42bae81f-e6ba-433d-ea3c-cda979a9b2d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m297.0/297.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def measure_execution_time(func, n, *args, **kwargs):\n",
        "    latencies = []\n",
        "\n",
        "    for _ in range(n):\n",
        "        start_time = time.time()\n",
        "        func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "\n",
        "        latency = end_time - start_time\n",
        "        latencies.append(latency)\n",
        "\n",
        "    latencies = np.array(latencies)\n",
        "\n",
        "    stats = {\n",
        "        'average_latency': np.mean(latencies),\n",
        "        'max_latency': np.max(latencies),\n",
        "        'min_latency': np.min(latencies),\n",
        "        'std_latency': np.std(latencies)\n",
        "    }\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "8kX_oBBI4sYD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "def get_chatgpt_answer(message: str, model, params={}) -> str:\n",
        "    chat_completion = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        **params\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "\n",
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic(\n",
        "    api_key=userdata.get(\"nebius_api_key\")\n",
        ")\n",
        "\n",
        "def get_anthropic_answer(message: str, model, params={'max_tokens': 1024}) -> str:\n",
        "    answer = client.messages.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": message,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        **params\n",
        "    )\n",
        "    return answer.content[0].text"
      ],
      "metadata": {
        "id": "vZ0xmTf25TF1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_to_test = [\n",
        "   \"gpt-3.5-turbo\",\n",
        "   \"gpt-4\",\n",
        "   \"gpt-4o\",\n",
        "   'gpt-4o-mini',\n",
        "   \"claude-3-opus-20240229\",\n",
        "   \"claude-3-sonnet-20240229\",\n",
        "   \"claude-3-haiku-20240307\",\n",
        "   \"claude-3-5-sonnet-20240620\"\n",
        "]\n",
        "\n",
        "for model in models_to_test:\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Model name {model}\")\n",
        "    if \"gpt\" in model:\n",
        "        completion_function = get_chatgpt_answer\n",
        "    else:\n",
        "        completion_function = get_anthropic_answer\n",
        "\n",
        "    print(measure_execution_time(\n",
        "        completion_function,\n",
        "        5,\n",
        "        \"What is the name of the cat from Shrek?\",\n",
        "        model,\n",
        "    ))\n",
        "\n",
        "    print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P093gQgP6UCR",
        "outputId": "20ecb3cc-5587-41b3-a14c-355cce5ddfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-3.5-turbo\n",
            "{'average_latency': 0.8572634220123291, 'max_latency': 1.2018272876739502, 'min_latency': 0.5498929023742676, 'std_latency': 0.21132660559943792}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4\n",
            "{'average_latency': 1.3319780826568604, 'max_latency': 1.8591728210449219, 'min_latency': 1.0715179443359375, 'std_latency': 0.2895837058446255}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4o\n",
            "{'average_latency': 1.4340587615966798, 'max_latency': 1.799485445022583, 'min_latency': 1.1727039813995361, 'std_latency': 0.20461863474660597}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4o-mini\n",
            "{'average_latency': 0.8255136489868165, 'max_latency': 1.3149795532226562, 'min_latency': 0.52872633934021, 'std_latency': 0.2659238645855903}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-opus-20240229\n",
            "{'average_latency': 6.670621252059936, 'max_latency': 7.630044460296631, 'min_latency': 4.185518026351929, 'std_latency': 1.2608159230239104}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-sonnet-20240229\n",
            "{'average_latency': 4.2181846618652346, 'max_latency': 5.192933797836304, 'min_latency': 2.964183807373047, 'std_latency': 0.7858535191549963}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-haiku-20240307\n",
            "{'average_latency': 1.3267467021942139, 'max_latency': 1.4615576267242432, 'min_latency': 1.0347704887390137, 'std_latency': 0.1652123840320664}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-5-sonnet-20240620\n",
            "{'average_latency': 3.3828349113464355, 'max_latency': 4.40121865272522, 'min_latency': 2.0159788131713867, 'std_latency': 0.8275362826567593}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are also parameters you can change to make latency a bit better. For example, if you want only a short sentence to be generated, you can set max_tokens. This speeds up response time a lot."
      ],
      "metadata": {
        "id": "zG0rGw187rno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models_to_test:\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Model name {model}\")\n",
        "    if \"gpt\" in model:\n",
        "        completion_function = get_chatgpt_answer\n",
        "    else:\n",
        "        completion_function = get_anthropic_answer\n",
        "\n",
        "    print(measure_execution_time(\n",
        "        completion_function,\n",
        "        5,\n",
        "        \"What is the name of the cat from Shrek?\",\n",
        "        model,\n",
        "        params={\n",
        "            \"max_tokens\": 10\n",
        "        }\n",
        "    ))\n",
        "\n",
        "    print(\"-\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn7lZM8g7rLT",
        "outputId": "2b0e95ba-7960-4b28-ca19-c8d3f53b00a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-3.5-turbo\n",
            "{'average_latency': 0.5668250560760498, 'max_latency': 0.8016211986541748, 'min_latency': 0.4170095920562744, 'std_latency': 0.13576578001880066}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4\n",
            "{'average_latency': 1.2162207126617433, 'max_latency': 1.757826328277588, 'min_latency': 0.9130477905273438, 'std_latency': 0.3195366842789354}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4o\n",
            "{'average_latency': 0.735747241973877, 'max_latency': 0.9551527500152588, 'min_latency': 0.4709610939025879, 'std_latency': 0.1668785130025617}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name gpt-4o-mini\n",
            "{'average_latency': 0.5356669425964355, 'max_latency': 0.6896207332611084, 'min_latency': 0.4326934814453125, 'std_latency': 0.10505698717267943}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-opus-20240229\n",
            "{'average_latency': 0.9264416217803955, 'max_latency': 0.9755957126617432, 'min_latency': 0.8509597778320312, 'std_latency': 0.05334933039670928}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-sonnet-20240229\n",
            "{'average_latency': 0.5098380088806153, 'max_latency': 0.5639405250549316, 'min_latency': 0.47768139839172363, 'std_latency': 0.033344852107309725}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-haiku-20240307\n",
            "{'average_latency': 0.4076198101043701, 'max_latency': 0.4381375312805176, 'min_latency': 0.3753323554992676, 'std_latency': 0.020479323556524355}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model name claude-3-5-sonnet-20240620\n",
            "{'average_latency': 0.4694643497467041, 'max_latency': 0.5548872947692871, 'min_latency': 0.40474534034729004, 'std_latency': 0.051646240099701306}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many other factors, which contribute to latency changes. For example lot of other people may be using the same model as you.\n",
        "\n",
        "In that case `gpt-4o-mini` can be slower than `gpt-4` just because of popularity at a certain time.  \n",
        "\n"
      ],
      "metadata": {
        "id": "bb2xRvSf78JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "This week we've learned:\n",
        "- How to use LangChain library.\n",
        "- How to add plugins to an help an LLM excel in more complex tasks.\n",
        "- How to create a vector database and how to interact with it.\n",
        "- About LLM latency and what affects it\n",
        "\n",
        "\n",
        "In this week's homework you'll:\n",
        "- Learn how to make ChatGPT nail high-school tests.\n",
        "- Learn to route between different LLMs depending on the task.\n",
        "- Create you own Gradio app to demo your LLM functionality."
      ],
      "metadata": {
        "id": "w-O2uAyrrTUd"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}