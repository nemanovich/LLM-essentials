{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nemanovich/LLM-essentials/blob/main/Structured_inputs_and_outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Engineering Essentials by Nebius Academy\n",
        "\n",
        "\n",
        "Author: Alex Umnov\n",
        "\n",
        "\n",
        "# Structured Inputs and Outputs\n",
        "\n",
        "Previously, we learnt how to prompt an LLM in such a way that it understands what you want from it and gives a relevant answer. In this notebook we'll continue this discussion by understanding\n",
        "\n",
        "* How to make prompts reusable by using **prompt templates**\n",
        "* How to ensure that an LLM creates its outputs in a convenient, easily parsable format\n",
        "\n",
        "Let's start by running some code which will help us in the whole notebook:"
      ],
      "metadata": {
        "id": "XXuQF3YXe0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -qU"
      ],
      "metadata": {
        "id": "vmxtcQ2de0DB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['NEBIUS_API_KEY'] = userdata.get(\"nebius_api_key\")\n",
        "\n",
        "nebius_client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        ")\n",
        "\n",
        "llama_model = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "def prettify_string(text, max_line_length=80):\n",
        "    \"\"\"Prints a string with line breaks at spaces to prevent horizontal scrolling.\n",
        "\n",
        "    Args:\n",
        "        text: The string to print.\n",
        "        max_line_length: The maximum length of each line.\n",
        "    \"\"\"\n",
        "\n",
        "    output_lines = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    for line in lines:\n",
        "        current_line = \"\"\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if len(current_line) + len(word) + 1 <= max_line_length:\n",
        "                current_line += word + \" \"\n",
        "            else:\n",
        "                output_lines.append(current_line.strip())\n",
        "                current_line = word + \" \"\n",
        "        output_lines.append(current_line.strip())  # Append the last line\n",
        "    return \"\\n\".join(output_lines)\n",
        "\n",
        "def answer_with_llm(prompt: str,\n",
        "                    system_prompt=\"You are a helpful assistant\",\n",
        "                    max_tokens=512,\n",
        "                    client=nebius_client,\n",
        "                    model=llama_model,\n",
        "                    prettify=True,\n",
        "                    temperature=None) -> str:\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            }\n",
        "        )\n",
        "\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt\n",
        "        }\n",
        "    )\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature\n",
        "    )\n",
        "\n",
        "    if prettify:\n",
        "        return prettify_string(completion.choices[0].message.content)\n",
        "    else:\n",
        "        return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "a1AxEa78e0DB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "L3wUdp21e0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an LLM-powered system, there's always a layer of prompting logic hidden from the user. For example, ChatGPT, Claude, Gemini and others have quite elaborate **system prompts** that set up rules and guardrails of LLM's communication with the user.\n",
        "\n",
        "However, in some cases a system prompting isn't a flexible enough mechanism. Imagine, for example,\n",
        "\n",
        "* a customer support bot that needs to be aware of the user's geography to give relevant answers about locally available products\n",
        "* a railway service support bot that needs to be aware of today's railway strikes and other calamities\n",
        "\n",
        "You'll likely need to insert this information in the middle of the prompt; and for such things, **prompt templates** are a great tool."
      ],
      "metadata": {
        "id": "xeHMXwGEe0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a **prompt template** is a template string like\n",
        "\n",
        "```python\n",
        "\"some fixed information {template placeholder 1}\n",
        "some more fixed information {template placeholder 2}\"\n",
        "```\n",
        "\n",
        "where the template placeholders are to be filled in just before an actual LLM call.\n",
        "\n",
        "Let's check several neat ways of wrapping this logic.\n",
        "\n",
        "First of all, you can write your own wrapper. In the example below, `m['content'].format(**kwargs)` allows to put as much formatting as you wish into the user's message."
      ],
      "metadata": {
        "id": "6i-MRNQMZvWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "class MessagesPromptTemplate():\n",
        "    messages: List[Dict]\n",
        "\n",
        "    def __init__(self, messages: List[Dict]):\n",
        "        self.messages = messages\n",
        "\n",
        "    def format(self, **kwargs):\n",
        "        return [\n",
        "            {\n",
        "                \"role\":  m['role'],\n",
        "                \"content\": m['content'].format(**kwargs)\n",
        "            }\n",
        "            for m in self.messages\n",
        "        ]"
      ],
      "metadata": {
        "id": "ZCv8sZ5Be0DC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = MessagesPromptTemplate(\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You only answer in rhymes\"},\n",
        "        {\"role\": \"user\", \"content\": \"Tell me about {city}\"}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "wrNIeKu3e0DD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.format(city=\"Paris\")"
      ],
      "metadata": {
        "id": "kjL-E-bGe0DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04da1379-acc1-41d3-8786-5ac095e7624e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Paris'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try calling an llm with different variables"
      ],
      "metadata": {
        "id": "jspgVhL0e0DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Paris\"),\n",
        "    model=llama_model\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "DuR5IqTGe0DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1837614-7477-4af7-a507-1661fac4cc22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Paris, the city of delight,\n",
            "The Eiffel Tower shines with great light.\n",
            "The Seine River flows, a wondrous sight,\n",
            "As art and love fill the morning light.\n",
            "\n",
            "The Louvre Museum, a treasure to see,\n",
            "Holds masterpieces of history.\n",
            "The Champs-Élysées, a street so fine and free,\n",
            "Invites you to stroll with glee.\n",
            "\n",
            "Montmartre's hills, a charming place to roam,\n",
            "With street performers and artists at home.\n",
            "The food, a culinary dream to call your own,\n",
            "Croissants and cheese, a taste to make you moan.\n",
            "\n",
            "In Paris, the city of love and desire,\n",
            "You'll find magic that sets your heart on fire.\n",
            "So come and visit, don't be shy or tired,\n",
            "In Paris, your heart will be inspired.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=prompt_template.format(city=\"Amsterdam\"),\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "_qP1h3ahe0DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aca8f78-fd01-470a-c05c-9a118b96cc74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amsterdam's a city so fine,\n",
            "In the Netherlands, it does shine.\n",
            "With canals and bridges, a wondrous sight,\n",
            "It's a place to visit, both day and night.\n",
            "\n",
            "The Rijksmuseum's there, with art on display,\n",
            "And the Anne Frank House, to learn and to say.\n",
            "The city's got charm, with its narrow streets too,\n",
            "And the people are friendly, with a \"hallo\" or two.\n",
            "\n",
            "The Vondelpark's green, with a peaceful little space,\n",
            "And the Jordaan neighborhood, with its quaint little face.\n",
            "You can take a boat tour, or ride a bike with glee,\n",
            "In Amsterdam, the city, that's full of energy.\n",
            "\n",
            "So if you ever visit, don't be shy or slow,\n",
            "Get out and explore, and let the city show,\n",
            "You its beauty and magic, its history and fun,\n",
            "In Amsterdam, the city, that's number one!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prompt template class we've written is very primitive and would fail if, for example, some keys aren't inputted.\n",
        "\n",
        "One of the good implementations of prompt templates can be found in LangChain [PromptTemplates](https://python.langchain.com/docs/concepts/prompt_templates/)"
      ],
      "metadata": {
        "id": "3nSqLBC2e0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -qU"
      ],
      "metadata": {
        "id": "m6nkay_qe0DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee38977c-129c-4b19-9ab4-d6a5e62d3575"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/442.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.8/442.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You only answer in rhymes\"),\n",
        "    (\"user\", \"Tell me about {city}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"city\": \"Madrid\"})"
      ],
      "metadata": {
        "id": "GeZEj5pCe0DE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1a493f-ae36-48f7-9b4e-e5ac1d56ddbe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You only answer in rhymes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about Madrid', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** You don't have to use LangChain llm calls or anything else, you can only take their PromptTemplate implementation.\n",
        "\n",
        "However, there's quiet a bit of useful code in that library."
      ],
      "metadata": {
        "id": "5zweywyae0DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import convert_to_openai_messages"
      ],
      "metadata": {
        "id": "8SyzAiTge0DF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "templated_messages = convert_to_openai_messages(prompt_template.invoke({\"city\": \"Madrid\"}).to_messages())\n",
        "templated_messages"
      ],
      "metadata": {
        "id": "xvE2qTQce0DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa5d6cb-e410-481f-eaf1-73bbb2308ac2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You only answer in rhymes'},\n",
              " {'role': 'user', 'content': 'Tell me about Madrid'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=templated_messages,\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "awT49JQve0DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f106206b-90c4-40d4-bc62-5d299555d228"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madrid's a city, so fine and so fair,\n",
            "Located in Spain, with culture to share.\n",
            "From Prado to Retiro, the sights are a delight,\n",
            "A city that's vibrant, day and night.\n",
            "\n",
            "The Royal Palace is grand, a must-see to explore,\n",
            "And the food, oh the food, is a culinary score.\n",
            "Tapas and paella, a taste sensation so fine,\n",
            "In Madrid, you'll dine, with a joy that's divine.\n",
            "\n",
            "The nightlife is lively, with music and cheer,\n",
            "In Malasaña, the fun, will last throughout the year.\n",
            "So visit Madrid, and let your spirit soar,\n",
            "In this city, you'll find, so much to adore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring LLM outputs\n",
        "\n",
        "In many cases you require not just a free text answer, but something particular you can use later in your system. For example, if you want your LLM to classify a customet's intent to later pass the conversation to a relevant department, you need to extract the particular intent class from the LLM's answer.\n",
        "\n",
        "To parse your LLM outputs conveniently, it's wise to structure them in a specific way. We've already discussed some prompting tricks in Topic 1; this time, we'll learn several more reliable ways of making the LLM abide a deisgnated output format."
      ],
      "metadata": {
        "id": "PLtqUw9ce0DF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic output structuring\n",
        "\n",
        "As a basic way to structure your output, you can \"ask\" an LLM to present the output in a specific format. For example:"
      ],
      "metadata": {
        "id": "Wb8TpmGZe0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Design one role play character\\'s name, class and a short description.\n",
        "Present it as a markdown list\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "liGqj3PKe0DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a885cad-0c14-4c5c-bf9b-f3e4fc5ac77d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Name:** Eira Shadowglow\n",
            "* **Class:** Moonlit Ranger\n",
            "* **Description:** A skilled and agile hunter with unparalleled accuracy, Eira navigates the shadows with ease, using her deep connection to nature to track and protect the innocent from the forces of darkness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this is quite good, it's not very reliable. A better way would be to show some examples to LLM so that it knows what we expect.\n",
        "\n",
        "These examples are known as **few-shot examples** and the prompting technique itself - as **few-shot prompting**."
      ],
      "metadata": {
        "id": "y7U9nI3Ae0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Design one role play character\\'s name, class and a short description. Present it as a markdown list.\\n'\\\n",
        "            \"Examples:\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Randalf the Yellow;\\n\"\\\n",
        "            \"- **Class:** Fire mage;\\n\"\\\n",
        "            \"- **Proficiency:** Pyro magic;\\n\"\\\n",
        "            \"- **Resistance:** Fire;\\n\"\\\n",
        "            \"\\n\"\\\n",
        "            \"- **Name:** Bonan;\\n\"\\\n",
        "            \"- **Class:** Barbarian;\\n\"\\\n",
        "            \"- **Proficiency:** Axe;\\n\"\\\n",
        "            \"- **Resistance:** Mental magic;\\n\"\\\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "I5lonLame0DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e083cf5-d743-46c0-f4b5-6eeb9b020e04"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Name:** Eriol the Shadow;\n",
            "- **Class:** Rogue;\n",
            "- **Proficiency:** Stealth and daggers;\n",
            "- **Resistance:** Poison;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, LLM captured the format pretty well.\n"
      ],
      "metadata": {
        "id": "PzygVBe0e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = nebius_client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Solve the following equation and output only the answer number without reasoning after \"Answer:\"\\n' \\\n",
        "            '123 * 321 = ?\\n' \\\n",
        "            'Answer:'\n",
        "        }\n",
        "    ],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "S4KXsxtZe0DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef246ed-b424-4331-9fd5-76b49c846e2e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ever though the answer isn't correct (LLMs are notoriously bad at arithmetics), the output structure is correct and easy to parse out."
      ],
      "metadata": {
        "id": "BtVmakLFe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can do even better."
      ],
      "metadata": {
        "id": "JNQ0ns6Oe0DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputing in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON fromat."
      ],
      "metadata": {
        "id": "_2G1O-w6e0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_output = nebius_client.chat.completions.create(\n",
        "    messages=[{'role': 'user', 'content': 'Design a role play character\\'s name, class and a short description in json format'}],\n",
        "    model=llama_model,\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "json_output"
      ],
      "metadata": {
        "id": "Q88QagE-e0DG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9422c233-1e79-4f70-f632-2dc8a553b735"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"class\": \"Shadow Assassin\", \"description\": \"A mysterious and deadly rogue with unparalleled stealth and agility, feared by his enemies and respected by his allies.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "m4KVrmlFe0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "b9-7ouAie0DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66408d73-96d1-44f9-e6c2-1f68b67a7df2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Eryndor Thorne',\n",
              " 'class': 'Shadow Assassin',\n",
              " 'description': 'A mysterious and deadly rogue with unparalleled stealth and agility, feared by his enemies and respected by his allies.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model to create a schema for our outputs:"
      ],
      "metadata": {
        "id": "tTrOKdoee0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = nebius_client.chat.completions.create(\n",
        "    model=llama_model,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    extra_body={\n",
        "        \"guided_json\": CharacterProfile.model_json_schema()\n",
        "    }\n",
        ")\n",
        "\n",
        "CharacterProfile.model_validate_json(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dki1ECRee0DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45235ef-9932-44a3-cfef-fb8781bee31e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharacterProfile(name='Eira Shadowglow', age=25, special_skills=['Stealth', 'Archery', 'Magic'], traits=['Curious', 'Independent', ' Resourceful'], character_class='Ranger', origin='Elven Realm')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "2viyoMVOe0DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to structure outputs is using examples\n",
        "\n",
        "Let's consider an example from a famous [MMLU dataset](https://huggingface.co/datasets/cais/mmlu):"
      ],
      "metadata": {
        "id": "TpeDDvNse0DH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Which of the following statements about Ethernets is typically FALSE?\"\n",
        "\n",
        "A = \"Ethernets use circuit switching to send messages.\"\n",
        "B = \"Ethernets use buses with multiple masters.\"\n",
        "C = \"Ethernet protocols use a collision-detection method to ensure that messages are transmitted properly.\"\n",
        "D = \"Networks connected by Ethernets are limited in length to a few hundred meters.\"\n",
        "\n",
        "correct_answer = \"A\""
      ],
      "metadata": {
        "id": "tygKuanMe0DI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally we want our LLM to solve this \"test\" by answering to us with a letter corresponding to the right answer. This will also make calculating metrics much easier. Let's see what would happen."
      ],
      "metadata": {
        "id": "s5vvF33Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "cG2GCODVe0DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55311ac3-672a-4988-ebbc-b00789c14774"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Ethernets use circuit switching to send messages.\n",
            "\n",
            "Explanation: Ethernets actually use packet switching, not circuit switching. Packet switching allows multiple devices to share the same communication channel, making it more efficient and scalable. Circuit switching, on the other hand, dedicates a communication channel to a single connection for the duration of the transmission.\n",
            "\n",
            "The other options are true:\n",
            "\n",
            "* B: Ethernets do use buses with multiple masters, which means that multiple devices can transmit data at the same time.\n",
            "* C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), use collision detection to ensure that messages are transmitted properly and to minimize collisions between devices.\n",
            "* D: Networks connected by Ethernets are indeed limited in length to a few hundred meters, typically up to 100 meters for twisted-pair Ethernet and up to 2 kilometers for fiber-optic Ethernet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it did output the right answer, but if we do a simple comparison, we'll get into trouble:"
      ],
      "metadata": {
        "id": "TaJNBEK7e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "hIadZJfVe0DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0e9490-c4e3-4317-bead-9ed5642809fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's teach our model to answer in the right way using so-called Few Shot Prompting also known as In-Context Learning. We essentially show the model some examples in the prompt to teach it in which format we want the answer to be"
      ],
      "metadata": {
        "id": "v6igGq25e0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Examples:\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Task:\n",
        "Answer the following question with one of the options listed below. Only ouput the answer in the same format as the examples.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "ahOoSsxhe0DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133a3e04-d191-4ade-a9c6-5d041b2835c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output == correct_answer"
      ],
      "metadata": {
        "id": "_wlgxkEue0DI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4cb232e-d130-4f37-c62e-09c9b824f450"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have observed that for some models the dialog format is actually a better way to structure the Few-Shot examples"
      ],
      "metadata": {
        "id": "uwEVq78Ve0DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: The IP protocol is primarily concerned with\n",
        "A: Routing packets through the network\n",
        "B: Reliable delivery of packets between directly connected machines\n",
        "C: Reliable delivery of large (multi-packet) messages between machines that are not necessarily directly connected\n",
        "D: Dealing with differences among operating system architectures\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: Which of the following is NOT a property of bitmap graphics?\n",
        "A: Fast hardware exists to move blocks of pixels efficiently\n",
        "B: Realistic lighting and shading can be done.\n",
        "C: All line segments can be displayed as straight.\n",
        "D: Polygons can be filled with solid colors and textures.\n",
        "Answer:\n",
        "Assistant: A\n",
        "\n",
        "User: Answer the following question with one of the options listed below.\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "Assitant:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WsQW4lNqe0DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efc7a01-ebd8-4223-89f8-65c352a34226"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Explanation: Ethernets use packet switching, not circuit switching, to send messages. Circuit switching is typically used in telephone networks, where a dedicated circuit is established between the caller and the receiver for the duration of the call. In contrast, Ethernets use packet switching, where data is broken into packets and transmitted independently through the network. \n",
            "\n",
            "So, the correct answer is A: Ethernets use circuit switching to send messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretically we don't even need to show the model relevant examples if we want it to learn the output formatting"
      ],
      "metadata": {
        "id": "rClIT520e0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = nebius_client.chat.completions.create(\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "Question: Choose the letter A\n",
        "A: A\n",
        "B: B\n",
        "C: C\n",
        "D: D\n",
        "Answer:\n",
        "A\n",
        "\n",
        "Question: Which is the biggest number?\n",
        "A: 1\n",
        "B: 2\n",
        "C: 3\n",
        "D: 4\n",
        "Answer:\n",
        "D\n",
        "\n",
        "Answer the following question with one of the options listed below\n",
        "Question: {question}\n",
        "A: {A}\n",
        "B: {B}\n",
        "C: {C}\n",
        "D: {D}\n",
        "Answer:\n",
        "\"\"\"}],\n",
        "    model=llama_model,\n",
        ").choices[0].message.content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bUyAniI1e0DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5647411a-38b0-4770-a764-88a25c6f801a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "\n",
            "Explanation: Ethernets use packet switching, not circuit switching. Circuit switching is typically used in telephone networks, where a dedicated circuit is established between the caller and the receiver for the duration of the call. In contrast, Ethernets use a packet-switching approach, where data is broken into packets and transmitted independently over the network. \n",
            "\n",
            "The other options are true: \n",
            "\n",
            "* B: Ethernets do use buses with multiple masters, allowing multiple devices to share the same communication channel.\n",
            "* C: Ethernet protocols, such as CSMA/CD (Carrier Sense Multiple Access with Collision Detection), do use collision-detection methods to ensure that messages are transmitted properly and to handle collisions when multiple devices try to transmit at the same time.\n",
            "* D: Networks connected by Ethernets are indeed limited in length to a few hundred meters, due to signal attenuation and other physical limitations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes you can confuse the model if you have examples from the distribution, which is different than your data's one. So for the best results try to match the distribution."
      ],
      "metadata": {
        "id": "dWhLwogce0DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling\n",
        "\n",
        "We can use tools in OpenAI api as well. Let's see how we can use web search with just the api:"
      ],
      "metadata": {
        "id": "sH-D2q9te0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python -qU"
      ],
      "metadata": {
        "id": "ML_0cLRre0DJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need a Tavily API key which you can get from [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "Then either use google's secret storage or put it into a file and upload it."
      ],
      "metadata": {
        "id": "g9r9qF5Qe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ['TAVILITY_API_KEY\"] = open(\".tavily_api_key\").read()\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "\n",
        "from tavily import TavilyClient\n",
        "\n",
        "tavily_client = TavilyClient()\n",
        "\n",
        "response = tavily_client.search(\"Who is Leo Messi?\", topic=\"general\")\n",
        "\n",
        "print(response['results'])"
      ],
      "metadata": {
        "id": "RjdV-AHye0DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44f064f-1fc2-445a-d17a-6b962b0f693b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'url': 'https://en.wikipedia.org/wiki/Lionel_Messi', 'title': 'Lionel Messi - Wikipedia', 'content': \"He is the most decorated player in the history of professional football having won 45 team trophies.Messi's records include most goals in a calendar year (91), most goals for a single club (672 for Barcelona), most goals in La Liga (474), most goal contributions in the FIFA World Cup (21), and most goal contributions in the Copa América (32).\", 'score': 0.752468, 'raw_content': None}, {'url': 'https://www.britannica.com/biography/Lionel-Messi', 'title': \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or ... - Britannica\", 'content': 'Lionel Messi scored 73 goals during the 2011–12 season while playing for FC Barcelona, breaking a 39-year-old record for single-season goals in a major European football league. Messi’s play continued to rapidly improve over the years, and by 2008 he was one of the most dominant players in the world, finishing second to Manchester United’sCristiano Ronaldo in the voting for the 2008 Ballon d’Or. In early 2009 Messi capped off a spectacular 2008–09 season by helping FC Barcelona capture the club’s first “treble” (winning three major European club titles in one season): the team won the La Liga championship, the Copa del Rey (Spain’s major domestic cup), and the Champions League title.', 'score': 0.5920305, 'raw_content': None}, {'url': 'https://www.transfermarkt.us/lionel-messi/profil/spieler/28003', 'title': 'Lionel Messi - Player profile 2025 | Transfermarkt', 'content': 'Image 1: Transfermarkt Image 15: Players Image 16: Lionel Messi Image 26: Lionel Messi Image 42: SpainSpain Position: Attack - Right Winger Foot:left Player agent:Relatives Current club: Image 43: Inter Miami CFInter Miami CFJoined: Jul 15, 2023 Contract expires:Dec 31, 2025 Outfitter:adidas Social-Media: Image 166: Lionel Messi Image 170: Messi Lionel Argentina 2024 Image 178: Lionel Messi Image 180: Lionel_Messi_Miami Image 193: Messi Image 205: Lionel Messi Image 206: Lionel Messi Image 207: Lionel Messi Image 208: Lionel Messi Image 209: Lionel Messi Image 210: Lionel Messi Image 211: Lionel Messi Image 212: Lionel Messi Image 213: Lionel Messi Image 214: Lionel Messi Image 215: Lionel Messi Image 216: Lionel Messi 2007 Image 217: Lionel Messi Image 230', 'score': 0.3289825, 'raw_content': None}, {'url': 'https://www.amazon.com/Who-Lionel-Messi-HQ-Now/dp/0593754824', 'title': 'Who Is Lionel Messi? (Who HQ Now) - Amazon.com', 'content': 'Image 36 Amazon Customer) *   Image 42 Amazon Customer ##### _5.0 out of 5 stars_ Perfect for new readers Reviewed in the United States on May 11, 2025 Format: PaperbackVerified Purchase Was trying to get my kid to read more! *   Image 44 Katelyn ##### _5.0 out of 5 stars_ Loved it Reviewed in the United States on April 30, 2025 Format: PaperbackVerified Purchase Both of my kids read it in one day-love this  Read more)   Helpful Report    *   Image 50 Houzhiqiang1228 ##### _5.0 out of 5 stars_ Good book about Messi Reviewed in the United States on January 31, 2025 Format: KindleVerified Purchase A good book about Messi, from child to now!', 'score': 0.3233165, 'raw_content': None}, {'url': 'https://www.intermiamicf.com/players/lionel-messi/', 'title': 'Lionel Messi | Inter Miami CF', 'content': 'Image 16: Atlanta site address Image 17: Austin site address Image 18: Charlotte site address Image 19: Chicago site address Image 20: Cincinnati site address Image 21: Colorado site address Image 22: Columbus site address Image 23: Dallas site address Image 25: Houston site address Image 26: LAFC site address Image 27: LA Galaxy site address Image 28: Miami site address Image 29: Minnesota site address Image 30: Montreal site address Image 31: Nashville site address Image 35: Orlando site address Image 36: Philadelphia site address Image 37: Portland site address Image 39: San Diego site address Image 40: San Jose site address Image 41: Seattle site address Image 44: Toronto site address Image 45: Vancouver site address', 'score': 0.14018343, 'raw_content': None}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define a `tool` description for client, so that the model knows how to use it.\n",
        "\n",
        "We will only expose `query` and `topic` parameters.\n",
        "\n",
        "We also need to write short descriptions to explain what the tool and the parameters are for. Note that it's not for you, but for the LLM :) So please make sure you provide a clear explanation.\n",
        "\n",
        "Tool usage is sort of an extension of \"JSON mode\" because in the end we get a dict of parameters, parsed from the JSON."
      ],
      "metadata": {
        "id": "IWspcVJxe0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"web-search\",\n",
        "            \"description\": \"Retrieves results from web search\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"What you search for\",\n",
        "                    },\n",
        "                    \"topic\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Search topic either 'general' or 'news'\",\n",
        "                        \"enum\": [\"general\", \"news\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is the name of the cat from Shrek?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "Y00ckl3Ie0DJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6defc638-9d17-400b-d23c-425f5f1b9578"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-097a6cca562f4d16a8dc16e15de7039b', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-32db6ab9762f4cfab5bbe74be50d15bc', function=Function(arguments='{\"query\": \"Shrek cat name\", \"topic\": \"general\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1753630131, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=267, total_tokens=299, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also try to ask for some news-worthy content to see if LLM decides on a different `topic`."
      ],
      "metadata": {
        "id": "xcU7jtcoe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked about the factual information, create a function call instead. If you already searched, use the results to give an answer.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What happened in London today?\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response"
      ],
      "metadata": {
        "id": "gx4QpV3Le0DK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78edd15a-dc07-4e81-c914-fbcecb2d9404"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-fed2c0b673c847588439707776235d07', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-4dd086e2cd2e4caf8e8d607806139e1f', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')], reasoning_content=None), stop_reason=128008)], created=1753630132, model='meta-llama/Llama-3.3-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=262, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can extract the function usage output from the result"
      ],
      "metadata": {
        "id": "n94smyzfe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_response.choices[0].message.tool_calls[0]"
      ],
      "metadata": {
        "id": "-AMA-dM9e0DK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb7adc7-bd1f-45c0-f38a-3ce81f9ee848"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessageToolCall(id='chatcmpl-tool-4dd086e2cd2e4caf8e8d607806139e1f', function=Function(arguments='{\"query\": \"London news today\", \"topic\": \"news\"}', name='web-search'), type='function')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering, why do we include tool usage in structured output topic.\n",
        "\n",
        "Thing is, you can also use this functionality to structure your output. You don't have to use a real function as your tool. Let's use our previous example"
      ],
      "metadata": {
        "id": "hDqCaytPe0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_rpg_character\",\n",
        "            \"description\": \"Creates a character based on attributes and description\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"name\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Name of the character\",\n",
        "                    },\n",
        "                    \"age\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Age of the character\",\n",
        "                    },\n",
        "                    \"special_skills\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of special skills of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"traits\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"description\": \"List of traits of the character\",\n",
        "                        \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                        }\n",
        "                    },\n",
        "                    \"character_class\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Class of the character\",\n",
        "                        \"enum\": [\"mage\", \"rogue\", \"barbarian\", \"knight\", \"paladin\"]\n",
        "                    },\n",
        "                    \"origin\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Origin of the character\",\n",
        "                        \"enum\": [\"human\", \"elf\", \"orc\", \"undead\"]\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"name\", \"age\", \"special_skills\", \"traits\", \"character_class\", \"origin\"],\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "ATabJqL4e0DK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "messages.append({\"role\": \"system\", \"content\": \"If you are asked to create a character, use `create_rpg_character` tool.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Generate a random character for my new session\"})\n",
        "chat_response = nebius_client.chat.completions.create(\n",
        "    messages=messages, tools=tools, model=llama_model\n",
        ")\n",
        "chat_response.choices[0].message.tool_calls[0].function.arguments"
      ],
      "metadata": {
        "id": "uFf2dqmPe0DK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7607d929-3278-4429-e3f5-f40af7cf1233"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"name\": \"Eryndor Thorne\", \"age\": \"27\", \"special_skills\": \"[\\\\\"stealth\\\\\", \\\\\"persuasion\\\\\"]\", \"traits\": \"[\\\\\"brave\\\\\", \\\\\"loyal\\\\\"]\", \"character_class\": \"rogue\", \"origin\": \"human\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice task. LLM Information extraction**\n",
        "\n"
      ],
      "metadata": {
        "id": "4bPMLxcse0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The goal of this task is to create a system, which extracts data about events from free text into a predictable format."
      ],
      "metadata": {
        "id": "7ZlJaY22e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional).\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "cEvtiZy6e0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in Nebius AI Studio's playground https://studio.nebius.com/playground.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output. But be careful. Though some models are easily prompted to output a JSON, please check the output format. It may contain excessive formatting, for example:\n",
        "<pre><code>```json\n",
        "{\"name\": \"InnovAI Summit 2023\", ...}\n",
        "```</pre></code>\n",
        "Actually, examining LLM outputs and their format is a must when working with them\n",
        "\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "eySv4YHWe0DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus points**:\n",
        "Try writing the solution using:\n",
        "- Structured JSON Output\n",
        "- Guiding JSON Output using Structures"
      ],
      "metadata": {
        "id": "vbJYFRc6e0DK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "zJspUelOe0DL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from typing import Optional, Union\n",
        "\n",
        "from pydantic import BaseModel, Field, model_validator\n",
        "\n",
        "SYSTEM_PROMPT: str = \"\"\"Act as a professional secretary. User input is a press release.\n",
        "Extract following data from that release: \"Event name\" (name), \"Event date\" (date),\n",
        "\"Number of participants\" (n_participants), \"Number of speakers\" (n_speakers),\n",
        "\"Attendance price\" (price). Be thorough and follow the format. If any fields is not\n",
        "mentioned in the text, put None in that field. 'Event date' must be valid and in format\n",
        "dd.mm.yyyy or dd.mm.yyyy-dd.mm.yyyy. Take into account that 'Event date' may be a range of dates.\n",
        "Price must be in format 'USD 10' or 'EUR 123'. Put in n_speakers amount of overtly mentioned\n",
        "speakers with names, including everyone who will be speaking at the event\n",
        "(except presenters, moderators or organizers).\"\"\"\n",
        "\n",
        "class PressReleaseData(BaseModel):\n",
        "    name: str\n",
        "    date: Optional[str] = Field(\n",
        "        default=None,\n",
        "        pattern=r\"^\\d{2}\\.\\d{2}\\.\\d{4}(\\-\\d{2}\\.\\d{2}\\.\\d{4})?$\",\n",
        "        examples=[\"12.25.2023\", \"10.01.2024-10.05.24\"],\n",
        "    )\n",
        "    n_participants: Optional[int] = Field(default=None, nullable=True)\n",
        "    n_speakers: Optional[Union[int, str]] = Field(default=None, nullable=True)\n",
        "    price: Optional[str] = Field(\n",
        "        default=None,\n",
        "        examples=[\"USD 250\", \"EUR 1024\"],\n",
        "    )\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def convert_zeros(self) -> 'PressReleaseData':\n",
        "        for field, value in self.__dict__.items():\n",
        "            if isinstance(value, int) and value == 0:\n",
        "                #None is a str because dataset has a wrong type - e.g `n_speakers: 'None'`\n",
        "                setattr(self, field, \"None\")\n",
        "        return self\n",
        "\n",
        "def parse_press_release(pr: str,\n",
        "                        model: str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
        "                        ) -> dict:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": str(pr)}\n",
        "    ]\n",
        "    completion = nebius_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.0,\n",
        "        extra_body={\n",
        "            \"guided_json\": PressReleaseData.model_json_schema()\n",
        "        },\n",
        "        #response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    content = completion.choices[0].message.content\n",
        "    return PressReleaseData.model_validate_json(content).model_dump()\n"
      ],
      "metadata": {
        "id": "fXP5oO41e0DL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "\n",
        "We've prepared a small dataset for you to test your prompt on. Provided you've written your function, try running the following code. At the end you also have an opportunity to look at the results in a table side-by-side in with_results.csv. Your goal is to get at least 60% of fields right.."
      ],
      "metadata": {
        "id": "rXJF_b3de0DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown -O press_release_extraction.csv https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv"
      ],
      "metadata": {
        "id": "dTVTe284e0DL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d4531f-cdc8-4280-8261-9a5f4a1042b1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.7.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://docs.google.com/spreadsheets/d/15IGdc3MV8864lxrLxsug0Ij480p76T1EAwBM7WGT_OI/export?format=csv\n",
            "To: /content/press_release_extraction.csv\n",
            "16.0kB [00:00, 25.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "pr_df = pandas.read_csv(\"press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "OJjcT2pSe0DL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        },
        "outputId": "a65eab9e-cd03-4ae7-eba8-1b4e3e71d577"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             pr_text  \\\n",
              "0  InnovAI Summit 2023: A Glimpse into the Future...   \n",
              "1  Press Dispatch: 'Artificial Mariners: Navigati...   \n",
              "2  FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...   \n",
              "3  Press Release: Cutting-Edge Innovations Debute...   \n",
              "4  Press Release: Innovative Minds Gather at \"AI ...   \n",
              "\n",
              "                                           pr_parsed  \n",
              "0  {\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...  \n",
              "1  {\"name\": \"Artificial Mariners: Navigatin' the ...  \n",
              "2  {\"name\": \"Annual Machine Learning Symposium 20...  \n",
              "3  {\"name\": \"AI Advancements Summit 2023\",\\n \"dat...  \n",
              "4  {\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d91f1ba-154f-4c50-834c-4abd9b82bee2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_text</th>\n",
              "      <th>pr_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>InnovAI Summit 2023: A Glimpse into the Future...</td>\n",
              "      <td>{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Press Dispatch: 'Artificial Mariners: Navigati...</td>\n",
              "      <td>{\"name\": \"Artificial Mariners: Navigatin' the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...</td>\n",
              "      <td>{\"name\": \"Annual Machine Learning Symposium 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Press Release: Cutting-Edge Innovations Debute...</td>\n",
              "      <td>{\"name\": \"AI Advancements Summit 2023\",\\n \"dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Press Release: Innovative Minds Gather at \"AI ...</td>\n",
              "      <td>{\"name\": \"AI Horizon 2023\",\\n \"date\": \"15.10.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d91f1ba-154f-4c50-834c-4abd9b82bee2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5d91f1ba-154f-4c50-834c-4abd9b82bee2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5d91f1ba-154f-4c50-834c-4abd9b82bee2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4db01f9a-ae5a-47a7-a043-953f4d075329\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4db01f9a-ae5a-47a7-a043-953f4d075329')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4db01f9a-ae5a-47a7-a043-953f4d075329 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pr_df",
              "summary": "{\n  \"name\": \"pr_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"pr_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\\n\\nCity of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\\n\\nEsteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\\n\\nThis year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\\n\\nAmong other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\\n\\nThe event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\\n\\nFor media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\",\n          \"Press Dispatch: 'Artificial Mariners: Navigatin' the AI Seas' - The Grand AI and Machine Learnin' Symposium of 2023\\n\\nOctober 12, 2023 - Tortuga Bay, The Spanish Main - Avast ye! Just a fortnight past, the shores of Tortuga Bay were graced with the grandest gatherin' of minds and marauders from across the seven seas. The event known far and wide as \\\"Artificial Mariners: Navigatin' the AI Seas\\\" did cast its anchor on the 8th and 9th of October, bringin' together a motley crew of over 2,000 sea dogs, scholars, and ship captains keen on decipherin' the mysteries of Artificial Intelligence and Machine Learnin'.\\n\\nWith the Jolly Roger flyin' high, the symposium boasted of tales and tools shared by the likes of the fearsome Dr. \\\"Blackwater\\\" Aria Kessler, known in the New World and Old for her dark arts in makin' machines mimic the mind. There were whispers amongst the crew about Dr. Jun \\\"Kraken\\\" Zhao, who spoke of harnessin' the monstrous power of calculations with nary a need for extra rum, savin' energy like a true sea scoundrel.\\n\\nNo gatherin' of this sort would be complete without explorin' the depths of ethical plunderin', led by the sharp-witted Dr. Sofia \\\"Siren\\\" \\u00c1lvarez. Her talk drew maps for navigatin' the fine line 'twixt progress and plunder, makin' sure the power of our newfound intelligence be used for the good of all, not just the few.\\n\\nBut shiver me timbers, it weren\\u2019t all just jabber! Tom\\u00e1s \\\"One-Eye\\\" Rivera, a cunning pirate with a penchant for codes and ciphers, put together a hands-on spectacle. This live code-crackin' session saw shipmates and buccaneers alike put their heads together, tacklin' problems that'd make even the saltiest of sea dogs sweat.\\n\\nAs the sun set on the final day, the renowned and somewhat mystical Dr. Emilia \\\"Seafarer\\\" van der Meer took to the stage, her eyes alight with visions of uncharted waters. Her words wove tales of futures where our trusty shipmates, the AI, be integral to weatherin' the storms ahead, safeguardin' not just our gold, but our lands and livelihoods.\\n\\n\\\"'Twas a rally like no other, fillin' our hearts with fire and our minds with dreams of treasures that lay beyond the horizons of man and machine,\\\" confessed an old tar as he prepared to disembark.\\n\\nAs the symposium closed its doors, the air was thick with plans and plots, the promise of alliances, and a shared quest to conquer the vast, unpredictable seas of Artificial Intelligence.\\n\\nFor those wishin' to re-live the adventure or who couldn\\u2019t sail with us this time around, visit [Symposium\\u2019s Mysterious Cove].\\n\\nContact for parley:\\nName: [Your trusty informant]\\nTitle: [Harbormaster of Information]\\nBird-mail: [Email]\\nMessage in a bottle: [Phone Number]\",\n          \"Press Release: \\\"AI for Equity Summit\\\" Champions Inclusivity in Technology\\n\\nOctober 18, 2023 \\u2014 The transformative \\\"AI for Equity Summit\\\" convened on October 15, 2023, marking an historic congregation of technological prowess dedicated to inclusivity and equitable advancements in artificial intelligence. The event welcomed over 3,000 attendees, each contributing a registration fee of $250, signifying their commitment to nurturing diversity in AI.\\n\\nSix illustrious speakers graced the summit's virtual stage, including Dr. Ayesha Khurram, renowned for her work in ethical AI, tech visionary Omar Svensson, Dr. Lola Adebayo, an advocate for minorities in STEM, Dr. Ji-hoon Park, known for his innovative algorithms against biases, Maria Navarro, a crusader for women in tech, and coding prodigy, Zane Kirschner, who is making significant strides in accessible AI for persons with disabilities.\\n\\nThe summit wasn't just about discussions; it was about making tangible strides. The highlight was the launch of the \\\"AI Equity Initiative,\\\" a fund that collected over $1.2 million from participant contributions, aimed at supporting tech education in underserved regions.\\n\\nWorkshops emphasized actionable strategies for dismantling systemic barriers in the tech industry. Dr. Adebayo's session on 'Intersectionality in AI Development' and Kirschner's workshop titled 'Coding Without Barriers' particularly stood out for their depth and engagement.\\n\\nMoreover, the \\\"AI for Equity Summit\\\" was proud to allocate 20% of all registration fees toward scholarships for students from underrepresented backgrounds to pursue AI studies.\\n\\nIn the wake of the event, a communiqu\\u00e9 was released, expressing a unified pledge among the participants and speakers to proactively include underrepresented groups in both the development of AI and the conversations around it.\\n\\nAttendees and interested parties are encouraged to stay informed through the summit's official channels for ongoing initiatives and future events.\\n\\nEnd of Release\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pr_parsed\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"{\\n  \\\"name\\\": \\\"InnovAI Summit 2023\\\",\\n  \\\"date\\\": \\\"05.11.2023\\\",\\n  \\\"n_participants\\\": 3500,\\n  \\\"n_speakers\\\": 4,\\n  \\\"price\\\": \\\"None\\\"\\n}\",\n          \"{\\\"name\\\": \\\"Artificial Mariners: Navigatin' the AI Seas\\\",\\n \\\"date\\\": \\\"08.10.2023-09.10.2023\\\",\\n \\\"n_participants\\\": 2000,\\n \\\"n_speakers\\\": 5,\\n \\\"price\\\":\\\"None\\\"}\",\n          \"{\\\"name\\\": \\\"AI for Equity Summit\\\",\\n \\\"date\\\": \\\"15.10.2023\\\",\\n \\\"n_participants\\\":  3000,\\n \\\"n_speakers\\\": 6,\\n \\\"price\\\": \\\"USD 250\\\"}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df.pr_parsed[0]"
      ],
      "metadata": {
        "id": "sDGJ1vKxe0DL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59a4c1c1-765e-4ffd-8148-4f300def1413"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\": \"05.11.2023\",\\n  \"n_participants\": 3500,\\n  \"n_speakers\": 4,\\n  \"price\": \"None\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "models = [\n",
        "    \"deepseek-ai/DeepSeek-V3-0324\",\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
        "]\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for model in models:\n",
        "  print(f\"Use LLM model {model}\")\n",
        "  for row in pr_df.itertuples():\n",
        "      parsed_release = parse_press_release(pr = row.pr_text, model = model)\n",
        "      parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "      golden = json.loads(row.pr_parsed)\n",
        "      for field, field_type in fields.items():\n",
        "          golden_field = golden[field]\n",
        "          parsed_field = parsed_release.get(field)\n",
        "          try:\n",
        "              parsed_field = field_type(parsed_field)\n",
        "          except (ValueError, TypeError):\n",
        "              pass\n",
        "          if golden_field == parsed_field:\n",
        "              correct_fields += 1\n",
        "          else:\n",
        "              print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "  print(f\"Correctly extracted {correct_fields} out of {5*len(pr_df)}\")\n",
        "  correct_fields = 0"
      ],
      "metadata": {
        "id": "RSvVEFCWe0DL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9616f62a-4de0-4f5f-b394-fcb33749296a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use LLM model deepseek-ai/DeepSeek-V3-0324\n",
            "For AI Horizon 2023 n_speakers None doesn't seem the same as None\n",
            "Correctly extracted 34 out of 35\n",
            "Use LLM model meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "For Artificial Mariners: Navigatin' the AI Seas n_speakers 4 doesn't seem the same as 5\n",
            "For Annual Machine Learning Symposium 2023 n_speakers 3 doesn't seem the same as 4\n",
            "For AI Advancements Summit 2023 price None doesn't seem the same as USD 950\n",
            "For AI Horizon 2023 n_speakers 4 doesn't seem the same as None\n",
            "Correctly extracted 31 out of 35\n",
            "Use LLM model meta-llama/Meta-Llama-3.1-405B-Instruct\n",
            "Correctly extracted 35 out of 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus points\n",
        "- Try and compare different ways of establishing the correct answer formatting\n",
        "- Try and compare different LLMs"
      ],
      "metadata": {
        "id": "lcGzo04oe0DL"
      }
    }
  ]
}