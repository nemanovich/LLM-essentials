{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nemanovich/LLM-essentials/blob/main/Week1_practice_session.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this week's practice session and homework you will:\n",
        "\n",
        "- Learn to make API calls to some of the popular llms,\n",
        "- Try and solve some tasks using llm api,\n",
        "- Explore all the different parameters for llm api inference,\n",
        "- Explore multimodality in llm api."
      ],
      "metadata": {
        "id": "dprPt7p8BNDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Text generative models (commonly refered to as LLMs) like ChatGPT or Claude are capable of doing great things, but you can't actually run them on your servers. So, there are two options:\n",
        "- Use open source models that are usually somewhat less capable than the top proprietary ones.\n",
        "- Use the most powerful models by API.\n",
        "\n",
        "In this part of the course we will concentrate on the second option."
      ],
      "metadata": {
        "id": "38910HM6i0V5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP_3EuB-A4oK"
      },
      "source": [
        "# Prerequisite: working with APIs basics\n",
        "\n",
        "We'll start by getting acquainted with the `requests` library. It's widely used to call url's, access api's, etc. You can jump to the next section **Tasks you can solve with LLMs** if you're already good with API"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URL requests basic\n",
        "\n",
        "There are two main types of request you can make to a server:\n",
        "\n",
        "- `POST` requests ask a web server to accept the data. For example, it can be used to update a database.\n",
        "\n",
        "- `GET` requets are used to receive information from the server.\n",
        "\n",
        "In this practice session we will only use `GET` requests, since we only want to call an api and receive a response."
      ],
      "metadata": {
        "id": "Zw5_t_q9H0rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's practice using `requests` by calling to the API of `catfact.ninja` web resource that return a random fact about cats."
      ],
      "metadata": {
        "id": "TpGbAmIP7sOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2T35A6SNA4oM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "r = requests.request(\"GET\", \"https://catfact.ninja/fact\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's always good to check the status code. The right one is `201`, others usually indicate various kinds of errors, see https://en.wikipedia.org/wiki/List_of_HTTP_status_codes for reference."
      ],
      "metadata": {
        "id": "qizljtLgLgtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r.status_code"
      ],
      "metadata": {
        "id": "QMHsNvIsLb8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5etOSfA4oN"
      },
      "source": [
        "By accessing the `content` attribute of our request we can see what answer the server returned us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMazTCcSA4oN"
      },
      "outputs": [],
      "source": [
        "r.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHqUVSLgA4oO"
      },
      "source": [
        "API's usually return information in some structured formats like JSON, YAML, ProtoBuf, etc. As we can see, in this case the response is JSON encoded. So let's decode it and see what we got!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjDlwgLyA4oO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json.loads(r.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If something goes wrong, the `get` function will return an error message with an error code. For example, if we send request to a non-existing address, we'll get:"
      ],
      "metadata": {
        "id": "dhS2lWL6BRkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.request(\"GET\", \"https://catfact.ninja/facta\")\n",
        "r.content, r.status_code"
      ],
      "metadata": {
        "id": "OHmGm89zAx3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy9oQ_9wA4oP"
      },
      "source": [
        "### Working with API\n",
        "\n",
        "The next two important things we need to learn is:\n",
        "\n",
        "1. How to authorize ourselves with an API key.\n",
        "\n",
        "2. How to request specific information from a server and not just a random thing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try that using [TheDogAPI](https://thedogapi.com/), a simple API created for educational purposes. It is free to try, but requires registration by email. It will provide you with an API key after registration.\n",
        "\n",
        "When you've registered, create a file `.dog-api-key` in the directory where you're working now and put the API key there"
      ],
      "metadata": {
        "id": "yIwrFSC3CotI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r05uNl7A4oP"
      },
      "outputs": [],
      "source": [
        "with open('.dog-api-key') as api_file:\n",
        "    dog_api_key = api_file.read().strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzANMnfEA4oP"
      },
      "source": [
        "TheDogAPI provides several services. For example, we can get a picture of a dog belonging to a specific breed. Let's analyze the code that is doing it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DLS6If6A4oP"
      },
      "outputs": [],
      "source": [
        "url = \"https://api.thedogapi.com/v1/images/search?format=json\"\n",
        "\n",
        "params = {\"breed_ids\": [10]}\n",
        "headers = {\n",
        "  'Content-Type': 'application/json',\n",
        "  'x-api-key': dog_api_key\n",
        "}\n",
        "\n",
        "response = requests.request(\n",
        "    \"GET\", url, headers=headers, params=params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're already familiar with the `requests` call. Now we also pass:\n",
        "\n",
        "- `headers` containing metadata such as the type of content and the API key.\n",
        "- `params` containing whatever parameters the API expects from us. In this case we need to provide the id of a breed (`10` means American Bulldog).\n",
        "\n",
        "If you want to know more about TheDogAPI, feel free to browse the documentation: https://documenter.getpostman.com/view/5578104/2s935hRnak#9e7e4cf9-0e0a-4258-8ace-ed1862843c96"
      ],
      "metadata": {
        "id": "KulF0IlQJ_us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if everything went ok. As a top-level verification we can check the status code, it should be 200"
      ],
      "metadata": {
        "id": "WYFqdClkK_wt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ArvDJeA4oQ"
      },
      "outputs": [],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMdqbX0A4oQ"
      },
      "source": [
        "If the code is 200, we can check the breed name and see a photo of an American Bulldog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMt9IMeZA4oQ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "dog_json = json.loads(response.text)\n",
        "print(\"Breed Name: \", dog_json[0]['breeds'][0]['name'])\n",
        "Image(dog_json[0]['url'], height=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5hAIBNeA4oQ"
      },
      "source": [
        "Congrats, we mastered calling the APIs and ready to get on with the real calls."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAv1hrnbA4oQ"
      },
      "source": [
        "# Tasks you can solve with LLMs\n",
        "\n",
        "In this section we will browse through several text-related generative tasks.\n",
        "\n",
        "Large Language Models (LLMs) can already solve a vast variety of different tasks, and they continue to improve. This illustration is a bit outdated, but still shows you main evolution paths of LLMs:\n",
        "\n",
        "<img src=\"https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.jpg?raw=true\"  width=\"60%\" height=\"60%\">\n",
        "\n",
        "\n",
        "There are many proprietary API offering for LLMs. Currently two of the most notables ones are OpenAIs and Anthropics, according to for example this benchmark: https://artificialanalysis.ai/.\n",
        "\n",
        "There're also models offered by Google, but their authentication system is notoriously complicated and we don't want to scare you too much already.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API basics\n",
        "\n",
        "OpenAI API requires registration and moreover it is commercial (but hopefully quite affordable at least on the scale required at our course). So please don't forget to register, acquire the API key and if you are following in collab put it in your secrets.\n",
        "\n",
        "With the API key we can just call the API, but OpenAI has a special convenient library:"
      ],
      "metadata": {
        "id": "6XawU5YxWKar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl1ieegGA4oR"
      },
      "outputs": [],
      "source": [
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to load the API key into it:"
      ],
      "metadata": {
        "id": "-CinnMm_ar3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VOIY7PLwA4oR"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to harness the power of GPT!\n",
        "\n",
        "Let's try something simple:"
      ],
      "metadata": {
        "id": "HXBs_6VybWi2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5mSgYF3HA4oS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e06cd9-a461-434c-a84a-e08512ec866e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BvOQBazN5UNAfRaae3ug2sBJajFJF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1753018271, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "chat_completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello world\"}]\n",
        ")\n",
        "chat_completion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the API call and the output.\n",
        "\n",
        "We need to input:\n",
        "\n",
        "1) **Model name**, you can choose from https://platform.openai.com/docs/models/, just mind the pricing.\n",
        "\n",
        "2) **Conversation history**. Unlike the web version of ChatGPT which memorizes your conversations, with API you need to provide the background story. We'll talk a more about it a bit later.\n",
        "\n",
        "The output contains a `message` of the *assistant* (that's how an OpenAI module presents itself) and indicates that the generation ended naturally because the message was complete (`\"finish_reason\"` is `\"stop\"`).\n",
        "\n",
        "The actual responce of the model can be obtained as:"
      ],
      "metadata": {
        "id": "BQN5M15FdC9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_answer = chat_completion.choices[0].message.content\n",
        "model_answer"
      ],
      "metadata": {
        "id": "gyUz9R1GdCXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a single shortcut function that takes a prompts and returns a completion suggested by the model:"
      ],
      "metadata": {
        "id": "lPC5gWzel6Ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl7H93h8A4oS"
      },
      "outputs": [],
      "source": [
        "# Write a function which for a given text returns ChatGPT response\n",
        "def get_chatgpt_answer(message: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUklE1fCA4oS"
      },
      "outputs": [],
      "source": [
        "def get_chatgpt_answer(message: str) -> str:\n",
        "    chat_completion = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}]\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_chatgpt_answer(\"Hello World!\")"
      ],
      "metadata": {
        "id": "ct6mJ9ZKQX-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The API also provides statistics of token `usage`. It can be important for you because OpenAI bills you based on the number of tokens its models process for you.\n",
        "\n",
        "Note that tokens are not the same as words, they are subword units of sort. You will deal with them in more detail in the homework.\n",
        "\n",
        "You can control the length of the model answer with the `max_tokens` parameter. For example:"
      ],
      "metadata": {
        "id": "4HuOkMbfgVCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    max_tokens=5,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please generate a long sentence\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "chat_completion"
      ],
      "metadata": {
        "id": "I6LcSlHskaT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that `\"finish_reason\"` is now `\"length\"` which means that `max_tokens` was reached before the generation stopped naturally.\n",
        "\n",
        "You can't ask the model to generate a text of arbitrary length. Each model has a restriction on a total number of tokens in **prompt + completion**. For example, it's 4,096 tokens for `gpt-3.5-turbo`. You can find the restrictions for each model at the [OpenAI model reference page](https://platform.openai.com/docs/models/gpt-3-5)."
      ],
      "metadata": {
        "id": "-J519RbhkamR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roles and communication history\n",
        "\n",
        "With `openai` library you can pass more than a prompt to the API. The `messages` parameter takes a list of messages with several possible roles, among them:\n",
        "\n",
        "- `\"user\"`, that's you.\n",
        "- `\"assistant\"`, a model's cue.\n",
        "- `\"system\"` used to pass our wishes regarding the assistant's tone of voice, restrictions etc.\n",
        "\n",
        "Let's look at a toy example. We can use `\"system\"` input to make a model only answer in rhymes:"
      ],
      "metadata": {
        "id": "vpnoz8P1lkRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant\" \\\n",
        "                \"and you only answer in rhymed sentences.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I need to write some code but \"\n",
        "                \"I'd prefer to go for a stroll.\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "pk67lS9tSXES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful way to utilize `system` is to ask for answers in a structured format, for example json."
      ],
      "metadata": {
        "id": "aZz_tLYH6FG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant\" \\\n",
        "                \"and you only answer in json format.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I need to write some code but \"\n",
        "                \"I'd prefer to go for a stroll.\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "fxIh7Oqn6Bui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way of changing the tone of voice of the model is showing some actual examples. Let's make our AI optimist provide short slogan-ish answers:"
      ],
      "metadata": {
        "id": "2JRsYKBZVY2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "response = openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\":\n",
        "         \"You are a helpful assistant and you are sure that Generative AI can solve any problem.\"},\n",
        "        {\"role\": \"user\", \"content\":\n",
        "         \"HR specialists spend too much time writing letters to the candidates.\"},\n",
        "        {\"role\": \"assistant\", \"content\":\n",
        "         \"We should create a Generative AI-powered mailing bot to help them!\"},\n",
        "        {\"role\": \"user\", \"content\":\n",
        "         \"I want to submit a paper to NeurIPS, but I don't have time to write it.\"},\n",
        "        {\"role\": \"assistant\", \"content\":\n",
        "         \"Just generate something with ChatGPT and become a NeurIPS star!\"},\n",
        "        {\"role\": \"user\", \"content\":\n",
        "         \"My child should draw a picture for tomorrow's art lesson, but doesn't have inspiration for this.\"},\n",
        "        {\"role\": \"assistant\", \"content\":\n",
        "         \"Don't worry, you can generate something with a diffusion model!\"},\n",
        "        {\"role\": \"user\", \"content\":\n",
        "         \"I need to write some code by I'd prefer to go for a stroll.\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "response.choices[0].message.content"
      ],
      "metadata": {
        "id": "vDRFsumX04d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we've taught our assistant to exhibit certain behavior without actual training just by showing some examples in a prompt. This is an example of **few-shot learning**.\n",
        "\n",
        "**Note 1**. Please don't mistake the recommendations of this assistant as actual advice. Applying AI to real-world problems should come in an ethical and safe way. For example, we need to make sure that an HR-mailing bot produces no offensive, biased or incoherent answers before it comes to production, and this can be very tricky.\n",
        "\n",
        "**Note 2**. Actually we can do the same thing in one `\"user\"` message, like:\n",
        "\n",
        "```\n",
        "You are a helpful assistant and you are sure that Generative AI can solve any problem.\n",
        "\n",
        "Q: HR specialists spend too much time writing letters to the candidates.\n",
        "A: We should create a Generative AI-powered mailing bot to help them!\n",
        "Q: I want to submit a paper to NeurIPS, but I don't have time to write it.\n",
        "A: Just generate something with ChatGPT and become a NeurIPS star!\n",
        "Q: My child should draw a picture for tomorrow's art lesson, but doesn't have inspiration for this.\n",
        "A: Don't worry, you can generate something with a diffusion model!\n",
        "Q: I need to write some code by I'd prefer to go for a stroll.\n",
        "A:\n",
        "```\n",
        "\n",
        "Moreover, this can actually work better, especially in some older models.\n",
        "\n",
        "We'll discuss few-shot learning more next week."
      ],
      "metadata": {
        "id": "HI8GyrH5Oi4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anthropic API Basics"
      ],
      "metadata": {
        "id": "z11HhJRxXtZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic -q"
      ],
      "metadata": {
        "id": "ZU44VdmyX9br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the following code, the request to Anthropic API is quite similar to OpenAI's with a bit of change."
      ],
      "metadata": {
        "id": "kFEKjpJ3Zym-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic(\n",
        "    api_key=userdata.get(\"anthropic_key\")\n",
        ")\n",
        "\n",
        "message = client.messages.create(\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Hello, Claude\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"claude-3-5-sonnet-20240620\",\n",
        ")\n",
        "print(message.content)"
      ],
      "metadata": {
        "id": "uIXJ0n4pZUBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a similar function for `get_anthropic_answer`"
      ],
      "metadata": {
        "id": "M6lfqDssaOPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Anthropic(\n",
        "    api_key=userdata.get(\"anthropic_key\")\n",
        ")\n",
        "\n",
        "def get_anthropic_answer(message: str) -> str:\n",
        "    answer = client.messages.create(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": message,\n",
        "            }\n",
        "        ],\n",
        "        model=\"claude-3-sonnet-20240229\",\n",
        "    )\n",
        "    return answer.content[0].text"
      ],
      "metadata": {
        "id": "ntqAWF4haODC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_anthropic_answer(\"Hello World!\")"
      ],
      "metadata": {
        "id": "M2ybie4Cawss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXyj9lcnA4oR"
      },
      "source": [
        "# Summarization\n",
        "\n",
        "\n",
        "For this task we will use openai api, so make sure that you have your key in `.open-ai-api-key` file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5D2U0YqA4oS"
      },
      "source": [
        "In the practice folder you can find a file `wikipedia_article.txt` with contents of Wikipedia article about paws.\n",
        "\n",
        "You can take a look at it yourself, but if you don't have much time, you can ask an LLM to summarise it for us. To do this, we feed to the API the text of the article together with a specific prompt indicating which task we want the model to solve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mmYZabkA4oT"
      },
      "outputs": [],
      "source": [
        "def summarise_with_gpt(text: str):\n",
        "    return get_chatgpt_answer(\n",
        "        f\"Write a short summary of the following text.\\n{text}\"\n",
        "    )\n",
        "\n",
        "def summarise_with_claude(text: str):\n",
        "    return get_anthropic_answer(\n",
        "        f\"Write a short summary of the following text.\\n{text}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two responses."
      ],
      "metadata": {
        "id": "uVnylhY9bGrg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7uFy6pAA4oT"
      },
      "outputs": [],
      "source": [
        "article = open(\"wikipedia_article.txt\").read()\n",
        "\n",
        "summarise_with_gpt(article)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = open(\"wikipedia_article.txt\").read()\n",
        "\n",
        "summarise_with_claude(article)"
      ],
      "metadata": {
        "id": "PSdQ_pcMbc1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YtB-RLBA4oT"
      },
      "source": [
        "You can check that this is really a coherent summary.\n",
        "\n",
        "If you modify the prompt, you can add a specific flavor to the summary. For example, you can ask the model to do it in simple English avoiding scientific terminology (try it!).\n",
        "\n",
        "You can also control the length of the summary using prompt like: `\"Summarize the following text in 2-3 sentences.\"` Typically, the more precise your prompt is, the more stable the results you get."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the differences between the two models aren't that big. We will focus on using OpenAI's models in the future. But you can choose either for any of the future tasks.\n",
        "\n",
        "Just be sure not to spend too much on the API calls :)"
      ],
      "metadata": {
        "id": "LwQq_l4VcEaB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCFN3_zfA4oT"
      },
      "source": [
        "# Translation\n",
        "\n",
        "Another task you can solve with ChatGPT is translation. Even though the quality of such translations is not the best, it's still fascinating that one model can do so many things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5NfGhRgA4oT"
      },
      "outputs": [],
      "source": [
        "# Write a function to translate to another language with the following signature\n",
        "# You can take summarise_with_gpt as an example\n",
        "\n",
        "def translate_with_gpt(text: str, target_language: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH83FQErA4oT"
      },
      "outputs": [],
      "source": [
        "def translate_with_gpt(text: str, target_language: str) -> str:\n",
        "    return get_chatgpt_answer(\n",
        "        f\"Translate the following text to {target_language}:\\n\"\\\n",
        "        f\"{text}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVojwnoA4oT"
      },
      "source": [
        "Now let's try it in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL5VpeYtA4oT"
      },
      "outputs": [],
      "source": [
        "translate_with_gpt(\n",
        "    \"I am a language model, nice to meet you!\",\n",
        "    target_language=\"Spanish\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHZYYDjkA4oU"
      },
      "source": [
        "In this simple example we can already see that it gives us a different translation, compared to a dedicated translation engine. Try pasting it to Google Translate and see the difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgYKxF49A4oU"
      },
      "source": [
        "Now let's try to implement a more complex pipeline with ChatGPT API. In the practice directory there's a file with an article about paws, but in japanese.\n",
        "\n",
        "Let's try to translate it to English using chatgpt API and then summarise and create a title for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRpv5nrcA4oU"
      },
      "outputs": [],
      "source": [
        "def create_title_with_chat_gpt(text: str) -> str:\n",
        "    return get_chatgpt_answer(\n",
        "        f\"Create a title for this text:\\n{text}\"\n",
        "    )\n",
        "\n",
        "def translate_and_summarise_with_chat_gpt(text):\n",
        "    print(\"Making a translate request\")\n",
        "    translated_text = translate_with_gpt(\n",
        "        text,\n",
        "        target_language=\"English\"\n",
        "    )\n",
        "    print('Making summarisation request')\n",
        "    summarized_text = summarise_with_gpt(translated_text)\n",
        "    print(\"making title request\")\n",
        "    title = create_title_with_chat_gpt(translated_text)\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"original\": text,\n",
        "        \"translated_text\": translated_text,\n",
        "        \"summary\": summarized_text\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJnVFcT4A4oU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "result = translate_and_summarise_with_chat_gpt(\n",
        "    open('wikipedia_article_japanese.txt').read()\n",
        ")\n",
        "display(f\"Title:\\n{result['title']}\")\n",
        "display(result['summary'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td-lpTeJA4oU"
      },
      "source": [
        "We hope that you're as curious as we are and you also wonder, what would happen if we do the same thing in a different order: summarise first, then translate. Let's try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDkt15CwA4oU"
      },
      "outputs": [],
      "source": [
        "def summarise_and_translate_with_chat_gpt(text):\n",
        "    print('Making summarisation request')\n",
        "    summarized_text = summarise_with_gpt(text)\n",
        "    print(\"making title request\")\n",
        "    title = create_title_with_chat_gpt(text)\n",
        "    print(\"Making a translate request\")\n",
        "    translated_text = translate_with_gpt(\n",
        "        summarized_text,\n",
        "        target_language=\"English\"\n",
        "    )\n",
        "    translated_title = translate_with_gpt(\n",
        "        title,\n",
        "        target_language=\"English\"\n",
        "    )\n",
        "    return {\n",
        "        \"title\": translated_title,\n",
        "        \"original\": text,\n",
        "        'summarized_text': summarized_text,\n",
        "        \"translated_text\": translated_text,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kizNkZ-A4oW"
      },
      "outputs": [],
      "source": [
        "result = summarise_and_translate_with_chat_gpt(\n",
        "    open('wikipedia_article_japanese.txt').read()\n",
        ")\n",
        "display(f\"Title:\\n{result['title']}\")\n",
        "display(result['summarized_text'])\n",
        "display(result['translated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtTnT2BkA4oW"
      },
      "source": [
        "As we can see, the result depends on the order of summarization and translation. Moreover, if we translate after summarization, the output is somewhat less coherent.\n",
        "\n",
        "On top of that, because we wrote the prompt in English, the summarization result is already in English.\n",
        "\n",
        "**Note**. Even if we don't change our pipeline and use the same request several times, ChatGPT's response might differ. This is due to the fact that we cannot directly controll the random state of the model with this API. You can make generation more reproducible using the parameter `temperature` of `openai.ChatCompletion.create`. It can take values between 0 and 2 with values closer to 0 making outputs more deterministic those above 0.8 making outputs more random and creative.\n",
        "\n",
        "If you run LLM locally (we'll do this in the second part of the course), you can make the responses deterministic by fixing the random states."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation parameters\n",
        "\n",
        "If you dive into the [API reference](https://platform.openai.com/docs/api-reference/chat) of for example OpenAI's text completion models, you can see a lot of different parameters, which you can set during the generation.\n",
        "Here's the ones we think are the most useful day-to-day:\n",
        "- `max_tokens` - the amount of tokens to generate. Can be useful if you want to generate an especially long completion, or if you want to only generate one work like \"True\" or \"False\",\n",
        "- `n` - the amount of generations you want to receive. Can be useful for checking consistency of the answer,\n",
        "- `response_format` - allows you to request json completion from the model,\n",
        "- `temperature` - a value between 0 and 2, where higher values increase the randomness of the answer. Can be useful for example when the answer requires creativity, or vice versa you want it to be very consistent,\n",
        "- `top_p` - an alternative sampling technique, the value you sent is the probability mass of tokens to consider. For example setting it to 0.1 will make it only use the first most probable tokens which probability sums up to 0.1.\n",
        "\n",
        "Let's try some of those parameters in action!"
      ],
      "metadata": {
        "id": "eI5L_hCHcaPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature\n",
        "\n",
        "The bigger the temperature, the more random is the output."
      ],
      "metadata": {
        "id": "qi6wU5OAfd-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "for _ in range(3):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"I need to write some code but \"\n",
        "                    \"I'd prefer to go for a stroll.\"},\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "dhmuAjSge8c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "for _ in range(3):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"I need to write some code but \"\n",
        "                    \"I'd prefer to go for a stroll.\"},\n",
        "        ],\n",
        "        temperature=1\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "bELAnTaffF32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top_p"
      ],
      "metadata": {
        "id": "1gTmE-eBfiuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "for _ in range(3):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"I need to write some code but \"\n",
        "                    \"I'd prefer to go for a stroll.\"},\n",
        "        ],\n",
        "        top_p=0.1\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "j2Lwuw3jfSo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"gpt-4o-mini\"\n",
        "for _ in range(3):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"I need to write some code but \"\n",
        "                    \"I'd prefer to go for a stroll.\"},\n",
        "        ],\n",
        "        top_p=0.5\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "sY5i2RWDfZ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured outputs\n",
        "\n",
        "Modern LLMs support outputting in a specific format, for example we can use \"JSON mode\" to force outputs to be in JSON format."
      ],
      "metadata": {
        "id": "99stviAwWjTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = open(\".open-ai-api-key\")\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"open_ai_api_key\")\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "non_json_output = client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'Design a role play character\\'s name, class and a short description'\n",
        "    }],\n",
        "    model=\"gpt-4o-mini\",\n",
        ").choices[0].message.content\n",
        "print(non_json_output)\n",
        "print(\"-\" * 100)\n",
        "\n",
        "json_output = client.chat.completions.create(\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'Design a role play character\\'s name, class and a short description in json format'\n",
        "    }],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    response_format={\"type\": \"json_object\"}\n",
        ").choices[0].message.content\n",
        "print(json_output)"
      ],
      "metadata": {
        "id": "ryrB2UfDXitX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is useful, because that'll make it much easier for you later to parse the outputs:"
      ],
      "metadata": {
        "id": "1T5XjuVIZJqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(json_output)"
      ],
      "metadata": {
        "id": "HOFWiZMFZNz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can go another step further and actually define a `pydantic` model for our outputs:"
      ],
      "metadata": {
        "id": "eB0ImQP5ZjzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CharacterProfile(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    special_skills: List[str]\n",
        "    traits: List[str]\n",
        "    character_class: str\n",
        "    origin: str\n",
        "\n",
        "completion = client.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o-2024-08-06\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Design a role play character\"}\n",
        "    ],\n",
        "    response_format=CharacterProfile,\n",
        ")\n",
        "\n",
        "completion.choices[0].message.parsed"
      ],
      "metadata": {
        "id": "r_jEZ2wiZpD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So no we have predefined format of outputs, which is easy to work with."
      ],
      "metadata": {
        "id": "FfXwmnGeacJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt caching\n",
        "\n",
        "There's a capability, which is present in Antropic's API only as a beta, but is fully implemented for Gemini.\n",
        "\n",
        "Prompt caching allows you to reuse you prompt's prefix if it was already present in previous request. This potentially saves a lof of computations.\n",
        "\n",
        "Read more [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#how-prompt-caching-works), there's quite a lot of limitations right now."
      ],
      "metadata": {
        "id": "ZMrpbcyFzswL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "# here we are inflating the article size so that it\n",
        "# hits 1024 token limit to start caching\n",
        "article = open(\"wikipedia_article.txt\").read() * 6\n",
        "\n",
        "client = Anthropic(\n",
        "    api_key=userdata.get(\"anthropic_key\"),\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Here's an article {article}, summarise it\",\n",
        "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
        "        }]\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "response = client.beta.prompt_caching.messages.create(\n",
        "    max_tokens=1024,\n",
        "    messages=messages,\n",
        "    model=\"claude-3-5-sonnet-20240620\"\n",
        ")\n",
        "response"
      ],
      "metadata": {
        "id": "4UPxsd9m0HZU",
        "outputId": "c1b4f923-7219-403d-cff1-b31259f5cb0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptCachingBetaMessage(id='msg_012FGL9C4RwSfZGvFTY5tRi2', content=[TextBlock(text=\"This article provides an overview of animal paws, focusing on their common characteristics and the animals that possess them. Here's a summary:\\n\\nPaw Characteristics:\\n1. Thin, pigmented, keratinized, hairless epidermis\\n2. Subcutaneous collagenous and adipose tissue forming pads\\n3. Heart-shaped metacarpal/palmar (forelimb) or metatarsal/plantar (rear limb) pad\\n4. Usually four load-bearing digital pads, sometimes five or six toes\\n5. Carpal pad on forelimb for extra traction\\n6. Horn-like, beak-shaped claws on each digit\\n7. Generally hairless, but some animals have fur on paw soles (e.g., red panda)\\n\\nAnimals with Paws:\\n1. Felids (cats, tigers)\\n2. Canids (dogs, foxes)\\n3. Lagomorphs (rabbits)\\n4. Bears and raccoons\\n5. Mustelids (weasels)\\n6. Rodents\\n\\nThe article notes that paws act as cushions for load-bearing limbs and that some animals, like rabbits, have sharp nails but no pads underneath their paws.\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=1944, cache_read_input_tokens=0, input_tokens=4, output_tokens=286))"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's try to alter the end of the prompt"
      ],
      "metadata": {
        "id": "mhcfDjA-29FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": response.content[0].text\n",
        "    }\n",
        ")\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Translate it to Japanese\"\n",
        "    }\n",
        ")\n",
        "\n",
        "client.beta.prompt_caching.messages.create(\n",
        "    max_tokens=1024,\n",
        "    messages=messages,\n",
        "    model=\"claude-3-5-sonnet-20240620\"\n",
        ")"
      ],
      "metadata": {
        "id": "lArxNAj62-3V",
        "outputId": "252d9f8b-a524-488b-89bf-acdb596666f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptCachingBetaMessage(id='msg_01A6fN4gUhkDu7YeTwYe1ZGK', content=[TextBlock(text=\"Here's a Japanese translation of the summary:\\n\\n\\n\\n\\n1. \\n2. \\n3. //\\n4. 456\\n5. \\n6. \\n7. \\n\\n\\n1. \\n2. \\n3. \\n4. \\n5. \\n6. \\n\\n\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=1944, input_tokens=299, output_tokens=382))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodality\n",
        "\n",
        "Modern LLMs also increasingly incorporate other modalities, usually Images. We can see that represented in Anthropic's and OpenAI's APIs. Let's try to feed out LLM evolution image to Claude and see what it things about it."
      ],
      "metadata": {
        "id": "qy_IbRsfgMaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "img_data = requests.get(\"https://github.com/Mooler0410/LLMsPracticalGuide/blob/main/imgs/tree.jpg?raw=true\").content\n",
        "\n",
        "base_64_encoded_data = base64.b64encode(img_data)\n",
        "base64_string = base_64_encoded_data.decode('utf-8')\n",
        "\n",
        "client = Anthropic(api_key=userdata.get(\"anthropic_key\"))\n",
        "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
        "\n",
        "\n",
        "message_list = [\n",
        "    {\n",
        "        \"role\": 'user',\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "                \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": base64_string}\n",
        "            },\n",
        "            {\"type\": \"text\", \"text\": \"What are the most important models in LLM evolution tree?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.messages.create(\n",
        "    model=MODEL_NAME,\n",
        "    max_tokens=2048,\n",
        "    messages=message_list\n",
        ")\n",
        "print(response.content[0].text)"
      ],
      "metadata": {
        "id": "7TVcrEk5haxP",
        "outputId": "706b8389-c8b3-42f7-b496-9a0feaccbe05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the evolutionary tree shown in the image, some of the most important models in the development of Large Language Models (LLMs) appear to be:\n",
            "\n",
            "1. GPT series: GPT-1, GPT-2, GPT-3, GPT-4, and their variations are prominently featured, showing their significant impact on the field.\n",
            "\n",
            "2. BERT: This model appears as one of the earlier branches, indicating its importance in the evolution of LLMs.\n",
            "\n",
            "3. T5: Shown as a major branch in the middle years of the evolution.\n",
            "\n",
            "4. LLaMA: Featured prominently in the most recent years, with multiple variations like LLaMA-2.\n",
            "\n",
            "5. PaLM: Another significant model in the recent developments.\n",
            "\n",
            "6. Claude: Shown as one of the latest developments in the tree.\n",
            "\n",
            "7. BLOOM: A large-scale model featured in the recent part of the evolution.\n",
            "\n",
            "8. Chinchilla: Positioned as an important recent development.\n",
            "\n",
            "9. ELECTRA and RoBERTa: Earlier models that contributed to the evolution.\n",
            "\n",
            "10. Galactica: A more recent model shown in the upper branches.\n",
            "\n",
            "These models represent key milestones in the development of LLMs, each contributing to advancements in the field. The tree also shows many other models and variations, indicating the rapid and diverse evolution of LLM technology over the years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's summarise\n",
        "\n",
        "**We learned:**\n",
        "\n",
        "* How to use `requests` library and call different type of API's\n",
        "\n",
        "* How to call GenAI api's and solve different task with them\n",
        "\n",
        "* Some of the caveats of using specific api's and how to tackle them\n",
        "\n",
        "\\\n",
        "\n",
        "To follow up on the topics we touched on in this seminar, welcome to this week's homework. We'll practice the following skills:\n",
        "\n",
        "* Summarizing long texts\n",
        "\n",
        "* Extracting information using LLMs\n",
        "\n",
        "* Understanding LLM's shortcomings"
      ],
      "metadata": {
        "id": "iTlAvQ_zD6Iv"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}